{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 14 â€“ Natural Language Processing with RNNs and Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains all the sample code and solutions to the exercises in chapter 14._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ageron/handson-mlp/blob/main/14_nlp_with_rnns_and_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-mlp/blob/main/14_nlp_with_rnns_and_attention.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project requires Python 3.10 or above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we using Colab or Kaggle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using Colab, the TorchMetrics library is not pre-installed so we must install it manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/983.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "if IS_COLAB:\n",
    "    %pip install -q torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need PyTorch â‰¥ 2.6.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging.version import Version\n",
    "import torch\n",
    "\n",
    "assert Version(torch.__version__) >= Version(\"2.6.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter can be very slow without a hardware accelerator, so if we can find one, let's use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's issue a warning if there's no hardware accelerator available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cpu\":\n",
    "    print(\"Neural nets can be very slow without a hardware accelerator.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware \"\n",
    "              \"accelerator.\")\n",
    "    if IS_KAGGLE:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in earlier chapters, let's define the default font sizes to make the figures prettier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the same `evaluate_tm()` and `train()` functions as in the previous chapters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "def evaluate_tm(model, data_loader, metric):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            metric.update(y_pred, y_batch)\n",
    "    return metric.compute()\n",
    "\n",
    "def train(model, optimizer, loss_fn, metric, train_loader, valid_loader,\n",
    "          n_epochs, patience=2, factor=0.5, epoch_callback=None):\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"max\", patience=patience, factor=factor)\n",
    "    history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.0\n",
    "        metric.reset()\n",
    "        model.train()\n",
    "        if epoch_callback is not None:\n",
    "            epoch_callback(model, epoch)\n",
    "        for index, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            metric.update(y_pred, y_batch)\n",
    "            train_metric = metric.compute().item()\n",
    "            print(f\"\\rBatch {index + 1}/{len(train_loader)}\", end=\"\")\n",
    "            print(f\", loss={total_loss/(index+1):.4f}\", end=\"\")\n",
    "            print(f\", {train_metric=:.2%}\", end=\"\")\n",
    "        history[\"train_losses\"].append(total_loss / len(train_loader))\n",
    "        history[\"train_metrics\"].append(train_metric)\n",
    "        val_metric = evaluate_tm(model, valid_loader, metric).item()\n",
    "        history[\"valid_metrics\"].append(val_metric)\n",
    "        scheduler.step(val_metric)\n",
    "        print(f\"\\rEpoch {epoch + 1}/{n_epochs},                      \"\n",
    "              f\"train loss: {history['train_losses'][-1]:.4f}, \"\n",
    "              f\"train metric: {history['train_metrics'][-1]:.2%}, \"\n",
    "              f\"valid metric: {history['valid_metrics'][-1]:.2%}\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will build and download pretty big models, we will need to free the GPU RAM regularly to avoid running out of space. For this, we will delete the models and tensors as we go, using the `del_vars()` function below. It deletes the given variables (if they exist) then calls Python's garbage collector, and it also calls `torch.cuda.empty_cache()` if we're using a CUDA GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def del_vars(variable_names=[]):\n",
    "    for name in variable_names:\n",
    "        try:\n",
    "            del globals()[name]\n",
    "        except KeyError:\n",
    "            pass  # ignore variables that have already been deleted\n",
    "    gc.collect()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: When running a Jupyter/Colab notebook, the output of each cell gets saved in the `Out` dictionary, so if the output of a cell is a large model or tensor, then it's not enough to delete the variable, you must also delete the output from the `Out` dictionary (e.g., by clearing the whole dictionary with `Out.clear()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Shakespearean Text Using a Character RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the Shakespeare data from Andrej Karpathy's [char-rnn project](https://github.com/karpathy/char-rnn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "def download_shakespeare_text():\n",
    "    path = Path(\"datasets/shakespeare/shakespeare.txt\")\n",
    "    if not path.is_file():\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://homl.info/shakespeare\"\n",
    "        urllib.request.urlretrieve(url, path)\n",
    "    return path.read_text()\n",
    "\n",
    "shakespeare_text = download_shakespeare_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "source": [
    "# extra code â€“ shows a short text sample\n",
    "print(shakespeare_text[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(shakespeare_text.lower()))\n",
    "\"\".join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_id = {char: index for index, char in enumerate(vocab)}\n",
    "id_to_char = {index: char for index, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_id[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_char[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def encode_text(text):\n",
    "    return torch.tensor([char_to_id[char] for char in text.lower()])\n",
    "\n",
    "def decode_text(char_ids):\n",
    "    return \"\".join([id_to_char[char_id.item()] for char_id in char_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20, 17, 24, 24, 27,  6,  1, 35, 27, 30, 24, 16,  2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = encode_text(\"Hello, world!\")\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'hello, world!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_text(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, window_length):\n",
    "        self.encoded_text = encode_text(text)\n",
    "        self.window_length = window_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_text) - self.window_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(\"dataset index out of range\")\n",
    "        end = idx + self.window_length\n",
    "        window = self.encoded_text[idx : end]\n",
    "        target = self.encoded_text[idx + 1 : end + 1]\n",
    "        return window, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=tensor([32, 27,  1, 14, 17,  1, 27, 30,  1, 26]), y=tensor([27,  1, 14, 17,  1, 27, 30,  1, 26, 27])\n",
      "    decoded: x='to be or n', y='o be or no'\n",
      "x=tensor([27,  1, 14, 17,  1, 27, 30,  1, 26, 27]), y=tensor([ 1, 14, 17,  1, 27, 30,  1, 26, 27, 32])\n",
      "    decoded: x='o be or no', y=' be or not'\n",
      "x=tensor([ 1, 14, 17,  1, 27, 30,  1, 26, 27, 32]), y=tensor([14, 17,  1, 27, 30,  1, 26, 27, 32,  1])\n",
      "    decoded: x=' be or not', y='be or not '\n",
      "x=tensor([14, 17,  1, 27, 30,  1, 26, 27, 32,  1]), y=tensor([17,  1, 27, 30,  1, 26, 27, 32,  1, 32])\n",
      "    decoded: x='be or not ', y='e or not t'\n",
      "x=tensor([17,  1, 27, 30,  1, 26, 27, 32,  1, 32]), y=tensor([ 1, 27, 30,  1, 26, 27, 32,  1, 32, 27])\n",
      "    decoded: x='e or not t', y=' or not to'\n",
      "x=tensor([ 1, 27, 30,  1, 26, 27, 32,  1, 32, 27]), y=tensor([27, 30,  1, 26, 27, 32,  1, 32, 27,  1])\n",
      "    decoded: x=' or not to', y='or not to '\n",
      "x=tensor([27, 30,  1, 26, 27, 32,  1, 32, 27,  1]), y=tensor([30,  1, 26, 27, 32,  1, 32, 27,  1, 14])\n",
      "    decoded: x='or not to ', y='r not to b'\n",
      "x=tensor([30,  1, 26, 27, 32,  1, 32, 27,  1, 14]), y=tensor([ 1, 26, 27, 32,  1, 32, 27,  1, 14, 17])\n",
      "    decoded: x='r not to b', y=' not to be'\n"
     ]
    }
   ],
   "source": [
    "# extra code â€“ a simple example using CharDataset\n",
    "to_be_dataset = CharDataset(\"To be or not to be\", window_length=10)\n",
    "for x, y in to_be_dataset:\n",
    "    print(f\"x={x}, y={y}\")\n",
    "    print(f\"    decoded: x={decode_text(x)!r}, y={decode_text(y)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 50\n",
    "batch_size = 512  # reduce if your GPU cannot handle such a large batch size\n",
    "train_set = CharDataset(shakespeare_text[:1_000_000], window_length)\n",
    "valid_set = CharDataset(shakespeare_text[1_000_000:1_060_000], window_length)\n",
    "test_set = CharDataset(shakespeare_text[1_060_000:], window_length)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2674,  0.5349,  0.8094],\n",
       "         [ 2.2082, -0.6380,  0.4617]],\n",
       "\n",
       "        [[ 0.3367,  0.1288,  0.2345],\n",
       "         [ 2.2082, -0.6380,  0.4617]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "embed = nn.Embedding(5, 3)  # 5 categories Ã— 3D embeddings\n",
    "embed(torch.tensor([[3, 2], [0, 2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training the Char-RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our Shakespeare model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layers=2, embed_dim=10, hidden_dim=128,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        embeddings = self.embed(X)\n",
    "        outputs, _states = self.gru(embeddings)\n",
    "        return self.output(outputs).permute(0, 2, 1)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = ShakespeareModel(len(vocab)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: the following code may take a while to run, especially without a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20,                      train loss: 1.6044, train metric: 51.28%, valid metric: 51.62%\n",
      "Epoch 2/20,                      train loss: 1.3852, train metric: 56.70%, valid metric: 53.03%\n",
      "Epoch 3/20,                      train loss: 1.3554, train metric: 57.44%, valid metric: 53.56%\n",
      "Epoch 4/20,                      train loss: 1.3412, train metric: 57.79%, valid metric: 53.53%\n",
      "Epoch 5/20,                      train loss: 1.3328, train metric: 58.01%, valid metric: 53.49%\n",
      "Epoch 6/20,                      train loss: 1.3271, train metric: 58.15%, valid metric: 53.79%\n",
      "Epoch 7/20,                      train loss: 1.3230, train metric: 58.26%, valid metric: 53.74%\n",
      "Epoch 8/20,                      train loss: 1.3200, train metric: 58.32%, valid metric: 54.57%\n",
      "Epoch 9/20,                      train loss: 1.3172, train metric: 58.39%, valid metric: 54.55%\n",
      "Epoch 10/20,                      train loss: 1.3153, train metric: 58.45%, valid metric: 53.95%\n",
      "Epoch 11/20,                      train loss: 1.3137, train metric: 58.49%, valid metric: 54.40%\n",
      "Epoch 12/20,                      train loss: 1.3049, train metric: 58.73%, valid metric: 54.96%\n",
      "Epoch 13/20,                      train loss: 1.3035, train metric: 58.77%, valid metric: 54.91%\n",
      "Epoch 14/20,                      train loss: 1.3025, train metric: 58.80%, valid metric: 54.84%\n",
      "Epoch 15/20,                      train loss: 1.3018, train metric: 58.82%, valid metric: 55.04%\n",
      "Epoch 16/20,                      train loss: 1.3011, train metric: 58.83%, valid metric: 54.91%\n",
      "Epoch 17/20,                      train loss: 1.3005, train metric: 58.84%, valid metric: 54.65%\n",
      "Epoch 18/20,                      train loss: 1.3000, train metric: 58.86%, valid metric: 54.74%\n",
      "Epoch 19/20,                      train loss: 1.2953, train metric: 59.00%, valid metric: 54.96%\n",
      "Epoch 20/20,                      train loss: 1.2946, train metric: 59.02%, valid metric: 55.00%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "xentropy = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters())\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\",\n",
    "                                 num_classes=len(vocab)).to(device)\n",
    "\n",
    "history = train(model, optimizer, xentropy, accuracy, train_loader, valid_loader,\n",
    "                n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"my_shakespeare_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # don't forget to switch the model to evaluation mode!\n",
    "text = \"To be or not to b\"\n",
    "encoded_text = encode_text(text).unsqueeze(dim=0).to(device)\n",
    "with torch.no_grad():\n",
    "    Y_logits = model(encoded_text)\n",
    "    predicted_char_id = Y_logits[0, :, -1].argmax().item()\n",
    "    predicted_char = id_to_char[predicted_char_id]  # correctly predicts \"e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Shakespearean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 1, 0, 2, 2]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "probs = torch.tensor([[0.5, 0.4, 0.1]])  # probas = 50%, 40%, and 10%\n",
    "samples = torch.multinomial(probs, replacement=True, num_samples=8)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def next_char(model, text, temperature=1):\n",
    "    encoded_text = encode_text(text).unsqueeze(dim=0).to(device)\n",
    "    with torch.no_grad():\n",
    "        Y_logits = model(encoded_text)\n",
    "        Y_probas = F.softmax(Y_logits[0, :, -1] / temperature, dim=-1)\n",
    "        predicted_char_id = torch.multinomial(Y_probas, num_samples=1).item()\n",
    "    return id_to_char[predicted_char_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_text(model, text, n_chars=80, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(model, text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be the state,\n",
      "and the heavens to the stronger than the state, and the strength of\n"
     ]
    }
   ],
   "source": [
    "print(extend_text(model, \"To be or not to b\", temperature=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be so deserved the grace\n",
      "to the sorrow is the earth of the truth of his face the \n"
     ]
    }
   ],
   "source": [
    "print(extend_text(model, \"To be or not to b\", temperature=0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to bmhf:my:r,k;s-h cqvvnfnfsut&-oq'ryoeen?x-hp:d,y&wv f3,dzrdzj-pilv?xpzh,fborp;'?$u\n"
     ]
    }
   ],
   "source": [
    "print(extend_text(model, \"To be or not to b\", temperature=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a stateful RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we have only used _stateless RNNs_: at each training iteration the model starts with a hidden state full of zeros, then it updates this state at each time step, and after the last time step, it throws the state away as it is not needed anymore. What if we instructed the RNN to preserve this final state after processing a training batch and use it as the initial state for the next training batch? This way the model could learn long-term patterns despite only backpropagating through short sequences. This is called a _stateful RNN_. Let's go over how to build one.\n",
    "\n",
    "The model itself requires very little change: we only need to add a new `hidden_states` attribute, initialized to `None`, then save the hidden states after each batch is processed, and use them as the initial hidden states for the next batch. Note that we must call `detach()` on these states to ensure we don't backpropagate over this training iteration's computation graph at the next iteration (this would cause an error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulShakespeareModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layers=2, embed_dim=10, hidden_dim=128,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.hidden_states = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        embeddings = self.embed(X)\n",
    "        outputs, hidden_states = self.gru(embeddings, self.hidden_states)\n",
    "        self.hidden_states = hidden_states.detach()\n",
    "        return self.output(outputs).permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difficulty with stateful RNNs is preparing the dataset. Indeed, a stateful RNN only makes sense if each input sequence in a batch starts exactly where the corresponding sequence in the previous batch left off. To be more precise, the _n_<sup>th</sup> window in batch _k_ must start exactly where the _n_<sup>th</sup> window in batch _k_ â€“ 1 stopped. For example, suppose the full encoded text is `[1, 2, 3, .., 59, 60, 61]` and you want to use a window length of 4, and a batch size of 5. The dataset could contain 3 batches like these, in this order:\n",
    "\n",
    "```\n",
    "Batch #1:\n",
    "X=[[1,2,3,4], [13,14,15,16], [25,26,27,28], [37,38,39,40], [49,50,51,52]]\n",
    "Y=[[2,3,4,5], [14,15,16,17], [26,27,28,29], [38,39,40,41], [50,51,52,53]]\n",
    "\n",
    "Batch #2:\n",
    "X=[[5,6,7,8], [17,18,19,20], [29,30,31,32], [41,42,43,44], [53,54,55,56]]\n",
    "y=[[6,7,8,9], [18,19,20,21], [30,31,32,33], [42,43,44,45], [54,55,56,57]]\n",
    "\n",
    "Batch #3:\n",
    "X=[[9,10,11,12], [21,22,23,24], [33,34,35,36], [45,46,47,48], [57,58,59,60]]\n",
    "y=[[10,11,12,13], [22,23,24,25], [34,35,36,37], [46,47,48,49], [58,59,60,61]]\n",
    "```\n",
    "\n",
    "Let's write a `StatefulCharDataset` class that organizes the data like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class StatefulCharDataset(Dataset):\n",
    "    def __init__(self, text, window_length, batch_size):\n",
    "        self.encoded_text = encode_text(text)\n",
    "        self.window_length = window_length\n",
    "        self.batch_size = batch_size\n",
    "        n_consecutive_windows = (len(self.encoded_text) - 1) // window_length\n",
    "        n_windows_per_slot = n_consecutive_windows // batch_size\n",
    "        self.length = n_windows_per_slot * batch_size\n",
    "        self.spacing = n_windows_per_slot * window_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(\"dataset index out of range\")\n",
    "        start = ((idx % self.batch_size) * self.spacing\n",
    "                 +(idx // self.batch_size) * self.window_length)\n",
    "        end = start + self.window_length\n",
    "        window = self.encoded_text[start : end]\n",
    "        target = self.encoded_text[start + 1 : end + 1]\n",
    "        return window, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the data loaders. Note that we must *not* shuffle the batches, even for the training set. We must also ensure that all batches have exactly the same number of windows, even the very last batch: for this reason, we must set `drop_last=True` when creating the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "stateful_train_set = StatefulCharDataset(shakespeare_text[:1_000_000],\n",
    "                                         window_length, batch_size)\n",
    "stateful_train_loader = DataLoader(stateful_train_set, batch_size=batch_size,\n",
    "                                   drop_last=True)\n",
    "stateful_valid_set = StatefulCharDataset(shakespeare_text[1_000_000:1_060_000],\n",
    "                                         window_length, batch_size)\n",
    "stateful_valid_loader = DataLoader(stateful_valid_set, batch_size=batch_size,\n",
    "                                   drop_last=True)\n",
    "stateful_test_set = StatefulCharDataset(shakespeare_text[1_060_000:],\n",
    "                                        window_length, batch_size)\n",
    "stateful_test_loader = DataLoader(stateful_test_set, batch_size=batch_size,\n",
    "                                  drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we should reset the hidden states at the start of each epoch. We could rewrite the whole training loop just for that, but it's cleaner to just pass a callback function to the `train()` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: the following cell may take a long time to run, especially without a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10,                      train loss: 2.4741, train metric: 29.85%, valid metric: 39.39%\n",
      "Epoch 2/10,                      train loss: 1.8786, train metric: 44.47%, valid metric: 44.68%\n",
      "Epoch 3/10,                      train loss: 1.6980, train metric: 49.23%, valid metric: 48.11%\n",
      "Epoch 4/10,                      train loss: 1.6011, train metric: 51.71%, valid metric: 49.94%\n",
      "Epoch 5/10,                      train loss: 1.5430, train metric: 53.16%, valid metric: 51.04%\n",
      "Epoch 6/10,                      train loss: 1.5043, train metric: 54.13%, valid metric: 51.94%\n",
      "Epoch 7/10,                      train loss: 1.4762, train metric: 54.85%, valid metric: 52.50%\n",
      "Epoch 8/10,                      train loss: 1.4540, train metric: 55.37%, valid metric: 53.06%\n",
      "Epoch 9/10,                      train loss: 1.4365, train metric: 55.84%, valid metric: 53.45%\n",
      "Epoch 10/10,                      train loss: 1.4226, train metric: 56.16%, valid metric: 53.97%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "stateful_model = StatefulShakespeareModel(len(vocab)).to(device)\n",
    "\n",
    "n_epochs = 10\n",
    "xentropy = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.NAdam(stateful_model.parameters())\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\",\n",
    "                                 num_classes=len(vocab)).to(device)\n",
    "\n",
    "def reset_hidden_states(model, epoch):\n",
    "    model.hidden_states = None\n",
    "\n",
    "history = train(stateful_model, optimizer, xentropy, accuracy, stateful_train_loader,\n",
    "                stateful_valid_loader, n_epochs, epoch_callback=reset_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(stateful_model.state_dict(), \"my_stateful_shakespeare_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try generating some text with our stateful RNN. The `hidden_states` are reset once at the beginning, then preserved across character generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_text_with_stateful_rnn(model, text, n_chars=80, temperature=1):\n",
    "    model.hidden_states = None\n",
    "    rnn_input = text\n",
    "    for _ in range(n_chars):\n",
    "        char = next_char(model, rnn_input, temperature)\n",
    "        text += char\n",
    "        rnn_input = char\n",
    "    return text + \"â€¦\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be the common\n",
      "that the strange in the common the commons and the first,\n",
      "and the sâ€¦\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "stateful_model.eval()\n",
    "print(extend_text_with_stateful_rnn(stateful_model, \"To be or not to b\",\n",
    "                                    temperature=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be the great and their earl\n",
      "and the seeming the finest thou shall have been to be\n"
     ]
    }
   ],
   "source": [
    "print(extend_text_with_stateful_rnn(stateful_model, \"To be or not to b\", temperature=0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be fout:\n",
      "then he shall foul hours of night from then, were\n",
      "desire? to him renerge\n"
     ]
    }
   ],
   "source": [
    "print(extend_text_with_stateful_rnn(stateful_model, \"To be or not to b\", temperature=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's free some GPU RAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Out.clear()  # clear Jupyter's `Out` variable which saves all the cell outputs\n",
    "del_vars([\"accuracy\", \"embed\", \"encoded\", \"encoded_text\", \"optimizer\", \"probs\",\n",
    "          \"samples\", \"x\", \"y\", \"shakespeare_text\", \"stateful_test_loader\",\n",
    "          \"stateful_train_loader\", \"Y_logits\", \"stateful_valid_loader\",\n",
    "          \"test_loader\", \"train_loader\", \"valid_loader\", \"xentropy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d09542b74e4b65b2a307203656e14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dee23ea2ef746c4a64a4812fb2c057d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5ab45fa40245c48c2461ed8e98fbb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8448a7350d7245bdaa185846a609e9cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/unsupervised-00000-of-00001.p(â€¦):   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac963354b37d4c978e19b2240d9a103c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90bc3e3b27844eb39136d21443291180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5459860fbe4fac881fa8b18e9a8553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "split = imdb_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "imdb_train_set, imdb_valid_set = split[\"train\"], split[\"test\"]\n",
    "imdb_test_set = imdb_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"'The Rookie' was a wonderful movie about the second chances life holds for us and also puts an emotional thought over the audience, making them realize that your dreams can come true. If you loved 'Remember the Titans', 'The Rookie' is the movie for you!! It's the feel good movie of the year and it is the perfect movie for all ages. 'The Rookie' hits a major home run!\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train_set[1][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train_set[1][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Lillian Hellman's play, adapted by Dashiell Hammett with help from Hellman, becomes a curious project to come out of gritty Warner Bros. Paul Lukas, reprising his Broadway role and winning the Best Actor Oscar, plays an anti-Nazi German underground leader fighting the Fascists, dragging his American wife and three children all over Europe before finding refuge in the States (via the Mexico border). They settle in Washington with the wife's wealthy mother and brother, though a boarder residing in the manor is immediately suspicious of the newcomers and spends an awful lot of time down at the German Embassy playing poker. It seems to take forever for this drama to find its focus, and when we realize what the heart of the material is (the wise, honest, direct refugees teaching the clueless, head-in-the-sand Americans how the world has suddenly changed), it seems a little patronizing--the viewer is quite literally put in the relatives' place, being lectured to. Lukas has several speeches in the third-act which undoubtedly won him the Academy Award, yet for the much of the picture he seems to do little but enter and exit, enter and exit. As his spouse, Bette Davis enunciates like nobody else and works her wide eyes to good advantage, but the role doesn't allow her much color. Their children (all with divergent accents!) are alternately humorous and annoying, and Geraldine Fitzgerald has a nothing role as a put-upon wife (and the disgruntled texture she brings to the part seems entirely wrong). The intent here was to tastefully, tactfully show us just because a (WWII-era) man may be German, that doesn't make him a Nazi sympathizer. We get that in the first few minutes; the rest of this tasteful, tactful movie is made up of exposition, defensive confrontation and, ultimately, compassion. It should be a heady mix, but instead it's rather dry-eyed and inert. ** from ****\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train_set[16][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train_set[16][\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Using the `tokenizers` Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a BPE Tokenizer on the IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "\n",
    "bpe_model = tokenizers.models.BPE(unk_token=\"<unk>\")\n",
    "bpe_tokenizer = tokenizers.Tokenizer(bpe_model)\n",
    "bpe_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
    "special_tokens = [\"<pad>\", \"<unk>\"]\n",
    "bpe_trainer = tokenizers.trainers.BpeTrainer(vocab_size=1000,\n",
    "                                             special_tokens=special_tokens)\n",
    "train_reviews = [review[\"text\"].lower() for review in imdb_train_set]\n",
    "bpe_tokenizer.train_from_iterator(train_reviews, bpe_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)), (',', (5, 6)), ('world', (7, 12)), ('!!!', (12, 15))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers.pre_tokenizers.Whitespace().pre_tokenize_str(\"Hello, world!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and Decoding Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_review = \"what an awesome movie! ðŸ˜Š\"\n",
    "bpe_encoding = bpe_tokenizer.encode(some_review)\n",
    "bpe_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'an', 'aw', 'es', 'ome', 'movie', '!', '<unk>']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_encoding.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[303, 139, 373, 149, 240, 211, 4, 1]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_token_ids = bpe_encoding.ids\n",
    "bpe_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.get_vocab()[\"what\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.token_to_id(\"what\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'ough'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.id_to_token(305)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'what an aw es ome movie !'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.decode(bpe_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4), (5, 7), (8, 10), (10, 12), (12, 15), (16, 21), (21, 22), (23, 24)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_encoding.offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Encoding(num_tokens=281, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=114, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=285, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.encode_batch(train_reviews[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\")\n",
    "bpe_tokenizer.enable_truncation(max_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[159, 402, 176, 246,  61, 782, 156, 737, 252,  42, 239,  51, 154, 460,\n",
       "         917,  17, 272, 156, 737, 576, 215, 976, 275,  42, 199,  44, 554,  42,\n",
       "         192, 585,  57, 160, 259, 170, 157, 143, 138, 159, 402,  11, 589, 152,\n",
       "           5, 819, 168, 230,   5, 521, 924, 981, 962, 250,  61,  10,  60, 426,\n",
       "         526, 959,  60, 138, 199, 150, 319,  15, 363, 141, 957, 694,  47, 696,\n",
       "          61, 875, 138, 960, 337, 414, 140, 157, 385, 174, 433, 161, 221, 145,\n",
       "         213,  17, 549,  15, 151,  10,  60,  55, 416, 146, 407, 144, 182, 303,\n",
       "         151, 141,  17, 138, 547, 538, 528, 768,  54, 335,  42, 203,  44, 270,\n",
       "          46, 153, 876, 141, 919, 233, 522, 172, 141, 719, 162, 807, 279,  17,\n",
       "         138,  45,  66,  55, 188, 989, 156, 378, 698, 301, 296, 689, 212, 558,\n",
       "         926, 148,  17,  44, 270,  46, 141,  47, 279, 302, 171, 152, 787,  15,\n",
       "         153, 522, 172, 766, 205, 156, 234, 677, 161, 139, 513, 146, 370, 251,\n",
       "         219, 162, 197, 162, 166,  50, 265,  47, 266, 177,  50,  10, 172, 502,\n",
       "         499, 210,  42, 163,  63, 137,  10,  60, 387,  15, 209,  50, 183, 155,\n",
       "         177,  51, 186, 774, 143, 221, 145,  10,  60, 176, 246,  61, 301, 141,\n",
       "         460,  50, 136, 355,  17, 138, 778, 141, 137, 534, 271,  43, 160, 265,\n",
       "          63, 290, 179, 157,  15, 153, 959,  60, 206, 360, 266, 148,  17,   5,\n",
       "         222, 606, 241, 246,   5, 141, 139, 145, 154,  54, 287, 160, 885, 148,\n",
       "         199,  15, 153, 141, 142, 994, 157, 182, 236, 637, 221,  47, 489, 156,\n",
       "         159, 402, 153, 718, 219, 162, 197, 162, 166,  26,  17,  23, 215, 156,\n",
       "         586,   0,   0,   0,   0],\n",
       "        [ 10, 138, 198, 289, 175,  10, 192,  42, 725, 355, 211, 311, 138, 964,\n",
       "         161, 139, 513, 493, 204, 207,  60, 182, 244, 153, 434, 679,  60, 139,\n",
       "          46, 854, 904, 733, 394, 138, 870, 415,  15, 839, 419, 433, 544,  46,\n",
       "         177, 508,  45, 142, 188,  60, 313, 576, 986,  17, 249, 206, 180, 541,\n",
       "          10, 142, 984, 138, 899, 489,  10,  15,  10, 138, 198, 289, 175,  10,\n",
       "         141, 138, 211, 182, 206, 584, 151,  10,  60, 138, 580, 321, 211, 156,\n",
       "         138, 967, 153, 151, 141, 138, 257, 684, 211, 182, 221, 281, 149,  17,\n",
       "          10, 138, 198, 289, 175,  10,  49, 377,  42, 239,  51, 154,  49, 240,\n",
       "         841,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0],\n",
       "        [289,  15, 209, 398, 177, 442, 173,  42, 321, 211,  34, 371,  15, 226,\n",
       "         370,  15, 137, 306, 583, 137, 185,  17, 291, 809,  10,  61,  42, 833,\n",
       "         384, 146, 142, 994, 151,  17,  50, 920, 151, 268,  60, 413,  15, 712,\n",
       "         399, 160,  15, 137, 727,  17, 151,  10,  60, 434, 295,  45, 543, 344,\n",
       "         735, 417, 525, 153, 525,  17,  55, 357, 440, 205, 153, 495, 382, 253,\n",
       "         303, 295,  45, 543, 344,  15, 209, 226, 152, 417,  17,  50, 434,  47,\n",
       "         203,  61, 173, 211, 192, 719,  44, 188,  57,  66, 144, 650,  15, 363,\n",
       "          50, 653,  10,  61, 370, 439,  47, 377, 173, 761, 153, 138, 331,  17,\n",
       "          51, 553,  47, 460, 183,  66, 387,  60, 138, 783, 404, 652, 137, 173,\n",
       "         137, 159, 221, 323,  17, 183,  10,  60,  42, 222, 530, 759, 477,  15,\n",
       "         209,  50,  47, 203,  61, 183, 910, 234, 331, 450, 394, 138, 919,  17,\n",
       "          50, 360, 266, 177,  47, 151, 201, 138,  61, 205, 156, 138, 211,  15,\n",
       "         363, 343, 252, 428, 403, 249,  50, 369, 190, 930, 138, 211,  17, 259,\n",
       "         244,  15, 291, 382, 253, 735, 404, 205, 849, 315,  17, 155, 174, 207,\n",
       "         838,  60, 180,  56, 142, 617,  60, 137, 138, 899, 163, 806,  15, 209,\n",
       "         141, 518, 293, 520, 146, 455, 201, 137, 173, 211,  17, 138, 331, 308,\n",
       "         226, 370, 598, 290, 541,  15, 152,  50, 369, 204, 938,  17,  56,  49,\n",
       "         371,  17, 173, 141, 299, 306, 583, 137, 185,  17, 317, 307,  15, 182,\n",
       "         250,  15, 548, 173, 211, 141, 226, 202,  66, 685, 150,  15, 151, 141,\n",
       "         735, 404,  17, 306, 838, 286, 182,  45, 900, 243,  50,  50,  50,  29,\n",
       "          22,  17,  24,  18,  24]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_encodings = bpe_tokenizer.encode_batch(train_reviews[:3])\n",
    "bpe_batch_ids = torch.tensor([encoding.ids for encoding in bpe_encodings])\n",
    "bpe_batch_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = torch.tensor([encoding.attention_mask\n",
    "                               for encoding in bpe_encodings])\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([281, 114, 285])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = attention_mask.sum(dim=-1)\n",
    "lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BBPE Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ðŸ˜Š emoji is represented as 4 bytes when using the UTF-8 Unicode encoding, so the `ByteLevel` pre-tokenizer represents it as 4 characters, each representing a byte. Spaces are converted to Ä ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ä what', (0, 4)),\n",
       " ('Ä an', (4, 7)),\n",
       " ('Ä awesome', (7, 15)),\n",
       " ('Ä movie', (15, 21)),\n",
       " ('!', (21, 22)),\n",
       " ('Ä Ã°ÅÄºÄ¬', (22, 24))]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers.pre_tokenizers.ByteLevel().pre_tokenize_str(some_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbpe_model = tokenizers.models.BPE(unk_token=\"<unk>\")\n",
    "bbpe_tokenizer = tokenizers.Tokenizer(bbpe_model)\n",
    "bbpe_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel()\n",
    "bbpe_trainer = tokenizers.trainers.BpeTrainer(vocab_size=1000,\n",
    "                                              special_tokens=special_tokens)\n",
    "bbpe_tokenizer.train_from_iterator(train_reviews, bbpe_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ä what',\n",
       " 'Ä an',\n",
       " 'Ä aw',\n",
       " 'es',\n",
       " 'ome',\n",
       " 'Ä movie',\n",
       " '!',\n",
       " 'Ä ',\n",
       " '<unk>',\n",
       " 'Å',\n",
       " 'Äº',\n",
       " '<unk>']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbpe_encoding = bbpe_tokenizer.encode(some_review)\n",
    "bbpe_tokens = bbpe_encoding.tokens\n",
    "bbpe_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[354, 216, 561, 148, 244, 232, 2, 107, 1, 125, 119, 1]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbpe_token_ids = bbpe_encoding.ids\n",
    "bbpe_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Ä what Ä an Ä aw es ome Ä movie ! Ä  Å Äº'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbpe_decoded = bbpe_tokenizer.decode(bbpe_token_ids)\n",
    "bbpe_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'what an awesome movie! ÅÄº'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbpe_decoded.replace(\" \", \"\").replace(\"Ä \", \" \").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_model = tokenizers.models.WordPiece(unk_token=\"<unk>\")\n",
    "wp_tokenizer = tokenizers.Tokenizer(wp_model)\n",
    "wp_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
    "wp_trainer = tokenizers.trainers.WordPieceTrainer(vocab_size=1000,\n",
    "                                                  special_tokens=special_tokens)\n",
    "wp_tokenizer.train_from_iterator(train_reviews, wp_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'an', 'aw', '##es', '##ome', 'movie', '!', '<unk>']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_encoding = wp_tokenizer.encode(some_review)\n",
    "wp_tokens = wp_encoding.tokens\n",
    "wp_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[443, 312, 635, 257, 354, 331, 4, 1]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_token_ids = wp_encoding.ids\n",
    "wp_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'what an aw ##es ##ome movie !'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_decoded = wp_tokenizer.decode(wp_token_ids)\n",
    "wp_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'what an awesome movie!'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_decoded.replace(\" ##\", \"\").replace(\" !\", \"!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_model = tokenizers.models.Unigram()\n",
    "unigram_tokenizer = tokenizers.Tokenizer(unigram_model)\n",
    "unigram_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
    "unigram_trainer = tokenizers.trainers.UnigramTrainer(\n",
    "    vocab_size=1000, special_tokens=special_tokens, unk_token=\"<unk>\")\n",
    "unigram_tokenizer.train_from_iterator(train_reviews, unigram_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'an', 'a', 'w', 'e', 'some', 'movie', '!', 'ðŸ˜Š']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_encoding = unigram_tokenizer.encode(some_review)\n",
    "unigram_tokens = unigram_encoding.tokens\n",
    "unigram_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[79, 37, 4, 40, 6, 70, 46, 74, 1]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_token_ids = unigram_encoding.ids[:10]\n",
    "unigram_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'what an a w e some movie !'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tokenizer.decode(unigram_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BBPE is used by models like GPT-2 and RoBERTa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77f840c35a3432fafacf662932c5a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7b9bb825a64ccc9b7cbfe88890f55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca5de5ee7d3435c905daf6efd462f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178d7a79e0ea47a6ab25492d43cacdfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c150c537d8ea437dbf5bb0cc1ff74c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "gpt2_tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_encoding = gpt2_tokenizer(train_reviews[:3], truncation=True,\n",
    "                               max_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView({'input_ids': [[14247, 35030, 1690, 423, 257, 1688, 8046, 13, 484, 1690, 1282, 503, 2045, 588, 257, 2646, 4676, 373, 2391, 4624, 319, 262, 3800, 357, 10508, 355, 366, 3847, 2802, 11074, 9785, 1681, 46390, 316, 338, 4571, 7622, 262, 2646, 6776, 11, 543, 318, 2592, 2408, 1201, 262, 4286, 4438, 683, 645, 1103, 4427, 13, 991, 11, 340, 338, 3621, 284, 804, 379, 329, 644, 340, 318, 13, 262, 16585, 1022, 285, 40302, 269, 5718, 290, 33826, 8803, 302, 44655, 318, 2407, 10457, 13, 262, 17262, 286, 511, 2776, 389, 6452, 13, 269, 5718, 318, 9623, 355, 1464, 11, 290, 302, 44655, 3011, 530, 286, 465, 1178, 8395, 284, 1107, 719, 29847, 1671, 1220, 6927, 1671, 11037, 72, 22127, 326, 1312, 1053, 1239, 1775, 4173, 64, 443, 7114, 338, 711, 11, 475, 1312, 3285, 326, 474, 323, 1803, 261, 477, 268, 338, 16711, 318, 17074, 13, 262, 4226, 318, 8131, 47370, 11, 290, 7622, 345, 25260, 13, 366, 22595, 46670, 1, 318, 281, 36005, 17774, 2646, 11, 290, 318, 7151, 329, 3016, 477, 3296, 286, 3800, 290, 3159, 29847, 1671, 1220, 6927, 1671, 11037, 22, 13, 19, 503, 286, 838], [470, 258, 12302, 6, 373, 257, 7932, 3807, 546, 262, 1218, 8395, 1204, 6622, 329, 514, 290, 635, 7584, 281, 7016, 1807, 625, 262, 5386, 11, 1642, 606, 6537, 326, 534, 10625, 460, 1282, 2081, 13, 611, 345, 6151, 705, 38947, 262, 5259, 504, 3256, 705, 1169, 12302, 6, 318, 262, 3807, 329, 345, 3228, 340, 338, 262, 1254, 922, 3807, 286, 262, 614, 290, 340, 318, 262, 2818, 3807, 329, 477, 9337, 13, 705, 1169, 12302, 6, 7127, 257, 1688, 1363, 1057, 0], [482, 11, 4360, 857, 326, 787, 428, 257, 922, 3807, 30, 4053, 11, 1662, 1107, 11, 259, 616, 4459, 13, 8117, 2125, 470, 257, 2187, 1256, 284, 4313, 340, 13, 72, 1043, 340, 845, 3105, 11, 1513, 6819, 11, 259, 1109, 13, 270, 338, 635, 20039, 2495, 881, 832, 290, 832, 13, 17618, 530, 290, 734, 547, 6454, 20039, 11, 4360, 407, 355, 881, 13, 72, 635, 2936, 428, 3807, 373, 2407, 1413, 88, 379, 1661, 11, 4758, 1312, 1422, 470, 1107, 892, 11414, 428, 2168, 290, 262, 2095, 13, 73, 14822, 24685, 20342, 5341, 262, 1388, 2089, 3516, 287, 428, 25168, 13, 258, 338, 257, 7709, 1576, 8674, 11, 4360, 1312, 2936, 339, 2826, 465, 2095, 1165, 625, 262, 1353, 13, 72, 4724, 326, 4197, 351, 262, 8216, 286, 262, 3807, 11, 4758, 561, 423, 587, 1049, 611, 1312, 550, 8288, 262, 3807, 13, 9541, 11, 8117, 547, 617, 2495, 2089, 530, 9493, 364, 13, 1501, 727, 410, 418, 29680, 5860, 287, 262, 3670, 2597, 11, 4360, 318, 1813, 1310, 284, 670, 351, 287, 428, 3807, 13, 1169, 2095, 468, 407, 1107, 12572, 11, 292, 1312, 550, 10719, 13, 1219, 880, 13, 5661, 318, 655, 616, 4459, 13, 1092, 1014, 11, 1640, 502, 11, 4514, 428, 3807, 318, 407, 450, 893, 7617, 11, 270, 318, 2495, 2089, 13, 1820, 3015, 329, 3223, 805, 46955, 25, 513, 13, 20, 14, 20]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14247, 35030, 1690, 423, 257, 1688, 8046, 13, 484, 1690]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_token_ids = gpt2_encoding[\"input_ids\"][0][:10]\n",
    "gpt2_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'stage adaptations often have a major fault. they often'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_tokenizer.decode(gpt2_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordPiece is used by models like BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75131e1532ca4b41bee1042300fb7be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1efa99e0984a7e8251f3a0ac34aecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b8a45fa76c4c5c9e515e5d6586892b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0683f7d1de44c6ae9e7469338cb120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert_tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_encoding = bert_tokenizer(train_reviews[:3], padding=True,\n",
    "                               truncation=True, max_length=500,\n",
    "                               return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2754, 17241,  2411,  2031,  1037,  2350,  6346,  1012,  2027,\n",
       "          2411,  2272,  2041,  2559,  2066,  1037,  2143,  4950,  2001,  3432,\n",
       "          2872,  2006,  1996,  2754,  1006,  2107,  2004,  1000,  2305,  2388,\n",
       "          1000,  1007,  1012, 11430, 11320, 11368,  1005,  1055,  3257,  7906,\n",
       "          1996,  2143,  4142,  1010,  2029,  2003,  2926,  3697,  2144,  1996,\n",
       "          3861,  3253,  2032,  2053,  2613,  4119,  1012,  2145,  1010,  2009,\n",
       "          1005,  1055,  3835,  2000,  2298,  2012,  2005,  2054,  2009,  2003,\n",
       "          1012,  1996,  6370,  2090,  2745, 19881,  1998,  5696, 20726,  2003,\n",
       "          3243,  8235,  1012,  1996, 10949,  1997,  2037,  3276,  2024, 11341,\n",
       "          1012, 19881,  2003, 10392,  2004,  2467,  1010,  1998, 20726,  4152,\n",
       "          2028,  1997,  2010,  2261,  9592,  2000,  2428,  2552,  1012,  1026,\n",
       "          7987,  1013,  1028,  1026,  7987,  1013,  1028,  1045, 18766,  2008,\n",
       "          1045,  1005,  2310,  2196,  2464, 11209, 20206,  1005,  1055,  2377,\n",
       "          1010,  2021,  1045,  2963,  2008,  6108,  2811,  2239,  5297,  1005,\n",
       "          1055,  6789,  2003, 11633,  1012,  1996,  5896,  2003, 11757,  9530,\n",
       "          6767,  7630,  3064,  1010,  1998,  7906,  2017, 16986,  1012,  1000,\n",
       "          2331,  6494,  2361,  1000,  2003,  2019,  8216,  2135, 14036,  2143,\n",
       "          1010,  1998,  2003,  6749,  2005,  3053,  2035,  4599,  1997,  2754,\n",
       "          1998,  3898,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,\n",
       "          1028,  1021,  1012,  1018,  2041,  1997,  2184,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  1005,  1996,  8305,  1005,  2001,  1037,  6919,  3185,  2055,\n",
       "          1996,  2117,  9592,  2166,  4324,  2005,  2149,  1998,  2036,  8509,\n",
       "          2019,  6832,  2245,  2058,  1996,  4378,  1010,  2437,  2068,  5382,\n",
       "          2008,  2115,  5544,  2064,  2272,  2995,  1012,  2065,  2017,  3866,\n",
       "          1005,  3342,  1996, 13785,  1005,  1010,  1005,  1996,  8305,  1005,\n",
       "          2003,  1996,  3185,  2005,  2017,   999,   999,  2009,  1005,  1055,\n",
       "          1996,  2514,  2204,  3185,  1997,  1996,  2095,  1998,  2009,  2003,\n",
       "          1996,  3819,  3185,  2005,  2035,  5535,  1012,  1005,  1996,  8305,\n",
       "          1005,  4978,  1037,  2350,  2188,  2448,   999,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  7929,  1010,  2021,  2515,  2008,  2191,  2023,  1037,  2204,\n",
       "          3185,  1029,  2092,  1010,  2025,  2428,  1010,  1999,  2026,  5448,\n",
       "          1012,  2045,  3475,  1005,  1056,  1037,  2878,  2843,  2000, 16755,\n",
       "          2009,  1012,  1045,  2179,  2009,  2200,  4030,  1010,  6945, 19426,\n",
       "          1010,  1999,  2755,  1012,  2009,  1005,  1055,  2036, 21425,  3492,\n",
       "          2172,  2083,  1998,  2083,  1012,  2193,  2028,  1998,  2048,  2020,\n",
       "          5399, 21425,  1010,  2021,  2025,  2004,  2172,  1012,  1045,  2036,\n",
       "          2371,  2023,  3185,  2001,  3243,  3409,  2100,  2012,  2335,  1010,\n",
       "          2029,  1045,  2134,  1005,  1056,  2428,  2228, 16142,  2023,  2186,\n",
       "          1998,  1996,  2839,  1012,  5076,  6904, 14844,  3248,  1996,  2364,\n",
       "          2919,  3124,  1999,  2023, 18932,  1012,  2002,  1005,  1055,  1037,\n",
       "         11519,  2438,  3364,  1010,  2021,  1045,  2371,  2002,  2209,  2010,\n",
       "          2839,  2205,  2058,  1996,  2327,  1012,  1045,  3984,  2008,  4906,\n",
       "          2007,  1996,  4309,  1997,  1996,  3185,  1010,  2029,  2052,  2031,\n",
       "          2042,  2307,  2065,  1045,  2018,  4669,  1996,  3185,  1012,  4606,\n",
       "          1010,  2045,  2020,  2070,  3492,  2919,  2028, 11197,  2015,  1012,\n",
       "          7779, 29536, 14540,  9541,  5651,  1999,  1996,  2516,  2535,  1010,\n",
       "          2021,  2003,  2445,  2210,  2000,  2147,  2007,  1999,  2023,  3185,\n",
       "          1012,  1996,  2839,  2038,  2025,  2428,  7964,  1010,  2004,  1045,\n",
       "          2018,  5113,  1012,  2821,  2092,  1012,  2023,  2003,  2074,  2026,\n",
       "          5448,  1012,  4312,  1010,  2005,  2033,  1010,  2096,  2023,  3185,\n",
       "          2003,  2025, 11113,  7274,  9067,  1010,  2009,  2003,  3492,  2919,\n",
       "          1012,  2026,  3789,  2005,  2601,  2386,  3523,  1024,  1017,  1012,\n",
       "          1019,  1013,  1019,   102]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_encoding[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_encoding[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2754, 17241,  2411,  2031,  1037,  2350,  6346,  1012,  2027,  2411,\n",
       "          2272,  2041,  2559,  2066,  1037,  2143,  4950,  2001,  3432,  2872,\n",
       "          2006,  1996,  2754,  1006,  2107,  2004,  1000,  2305,  2388,  1000,\n",
       "          1007,  1012, 11430, 11320, 11368,  1005,  1055,  3257,  7906,  1996,\n",
       "          2143,  4142,  1010,  2029,  2003,  2926,  3697,  2144,  1996,  3861,\n",
       "          3253,  2032,  2053,  2613,  4119,  1012,  2145,  1010,  2009,  1005,\n",
       "          1055,  3835,  2000,  2298,  2012,  2005,  2054,  2009,  2003,  1012,\n",
       "          1996,  6370,  2090,  2745, 19881,  1998,  5696, 20726,  2003,  3243,\n",
       "          8235,  1012,  1996, 10949,  1997,  2037,  3276,  2024, 11341,  1012,\n",
       "         19881,  2003, 10392,  2004,  2467,  1010,  1998, 20726,  4152,  2028,\n",
       "          1997,  2010,  2261,  9592,  2000,  2428,  2552,  1012,  1026,  7987,\n",
       "          1013,  1028,  1026,  7987,  1013,  1028,  1045, 18766,  2008,  1045,\n",
       "          1005,  2310,  2196,  2464, 11209, 20206,  1005,  1055,  2377,  1010,\n",
       "          2021,  1045,  2963,  2008,  6108,  2811,  2239,  5297,  1005,  1055,\n",
       "          6789,  2003, 11633,  1012,  1996,  5896,  2003, 11757,  9530,  6767,\n",
       "          7630,  3064,  1010,  1998,  7906,  2017, 16986,  1012,  1000,  2331,\n",
       "          6494,  2361,  1000,  2003,  2019,  8216,  2135, 14036,  2143,  1010,\n",
       "          1998,  2003,  6749,  2005,  3053,  2035,  4599,  1997,  2754,  1998,\n",
       "          3898,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,\n",
       "          1021,  1012,  1018,  2041,  1997,  2184,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [ 1005,  1996,  8305,  1005,  2001,  1037,  6919,  3185,  2055,  1996,\n",
       "          2117,  9592,  2166,  4324,  2005,  2149,  1998,  2036,  8509,  2019,\n",
       "          6832,  2245,  2058,  1996,  4378,  1010,  2437,  2068,  5382,  2008,\n",
       "          2115,  5544,  2064,  2272,  2995,  1012,  2065,  2017,  3866,  1005,\n",
       "          3342,  1996, 13785,  1005,  1010,  1005,  1996,  8305,  1005,  2003,\n",
       "          1996,  3185,  2005,  2017,   999,   999,  2009,  1005,  1055,  1996,\n",
       "          2514,  2204,  3185,  1997,  1996,  2095,  1998,  2009,  2003,  1996,\n",
       "          3819,  3185,  2005,  2035,  5535,  1012,  1005,  1996,  8305,  1005,\n",
       "          4978,  1037,  2350,  2188,  2448,   999,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [ 7929,  1010,  2021,  2515,  2008,  2191,  2023,  1037,  2204,  3185,\n",
       "          1029,  2092,  1010,  2025,  2428,  1010,  1999,  2026,  5448,  1012,\n",
       "          2045,  3475,  1005,  1056,  1037,  2878,  2843,  2000, 16755,  2009,\n",
       "          1012,  1045,  2179,  2009,  2200,  4030,  1010,  6945, 19426,  1010,\n",
       "          1999,  2755,  1012,  2009,  1005,  1055,  2036, 21425,  3492,  2172,\n",
       "          2083,  1998,  2083,  1012,  2193,  2028,  1998,  2048,  2020,  5399,\n",
       "         21425,  1010,  2021,  2025,  2004,  2172,  1012,  1045,  2036,  2371,\n",
       "          2023,  3185,  2001,  3243,  3409,  2100,  2012,  2335,  1010,  2029,\n",
       "          1045,  2134,  1005,  1056,  2428,  2228, 16142,  2023,  2186,  1998,\n",
       "          1996,  2839,  1012,  5076,  6904, 14844,  3248,  1996,  2364,  2919,\n",
       "          3124,  1999,  2023, 18932,  1012,  2002,  1005,  1055,  1037, 11519,\n",
       "          2438,  3364,  1010,  2021,  1045,  2371,  2002,  2209,  2010,  2839,\n",
       "          2205,  2058,  1996,  2327,  1012,  1045,  3984,  2008,  4906,  2007,\n",
       "          1996,  4309,  1997,  1996,  3185,  1010,  2029,  2052,  2031,  2042,\n",
       "          2307,  2065,  1045,  2018,  4669,  1996,  3185,  1012,  4606,  1010,\n",
       "          2045,  2020,  2070,  3492,  2919,  2028, 11197,  2015,  1012,  7779,\n",
       "         29536, 14540,  9541,  5651,  1999,  1996,  2516,  2535,  1010,  2021,\n",
       "          2003,  2445,  2210,  2000,  2147,  2007,  1999,  2023,  3185,  1012,\n",
       "          1996,  2839,  2038,  2025,  2428,  7964,  1010,  2004,  1045,  2018,\n",
       "          5113,  1012,  2821,  2092,  1012,  2023,  2003,  2074,  2026,  5448,\n",
       "          1012,  4312,  1010,  2005,  2033,  1010,  2096,  2023,  3185,  2003,\n",
       "          2025, 11113,  7274,  9067,  1010,  2009,  2003,  3492,  2919,  1012,\n",
       "          2026,  3789,  2005,  2601,  2386,  3523,  1024,  1017,  1012,  1019,\n",
       "          1013,  1019]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code â€“ shows how to drop the special tokens\n",
    "bert_encoding = bert_tokenizer(train_reviews[:3], padding=True,\n",
    "                               truncation=True, max_length=500,\n",
    "                               add_special_tokens=False, return_tensors=\"pt\")\n",
    "bert_encoding[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram LM is used by models like ALBERT, T5, and XLM-R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f276e120bb441893c1b703ecd374b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe78c623e20428ea19a96faaf678ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281652a20edc4587abf2b41b5459c553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7aecbeefb14f51b3c4ab415d4429d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  876,  5004,    18,   478,    57,    21,   394,  4173,     9,    59,\n",
       "           478,   340,    70,   699,   101,    21,   171,  3336,    23,  1659,\n",
       "          1037,    27,    14,   876,    13,     5,  4289,    28,    13,     7,\n",
       "          4893,   449,     7,     6,     9, 12508,  1612,  5909,    22,    18,\n",
       "          1400,  8968,    14,   171,  2481,    15,    56,    25,  1118,  1956,\n",
       "           179,    14,  2151,  1434,    61,    90,   683,  2404,     9,   174,\n",
       "            15,    32,    22,    18,  2210,    20,   361,    35,    26,    98,\n",
       "            32,    25,     9,    14,  5427,   128,   832, 22427,    17,  4479,\n",
       "         24604,    25,  1450,  7472,     9,    14, 12289,    16,    66,  1429,\n",
       "            50, 12891,     9, 22427,    25, 10356,    28,   550,    15,    17,\n",
       "         24604,  3049,    53,    16,    33,   310, 11285,    20,   510,   601,\n",
       "             9,     1,  5145,    13,   118,     1,  5145,    13,   118,     1,\n",
       "            49, 14586,    30,    31,    22,   195,   243,   541, 16216, 18863,\n",
       "            22,    18,   418,    15,    47,    31,   990,    30,  3361,   901,\n",
       "           218,  3675,    22,    18,  5004,    25, 10763,     9,    14,  3884,\n",
       "            25, 13003,  1065, 16261,  1427,    15,    17,  8968,    42, 19523,\n",
       "             9,    13,     7, 13921, 16514,     7,    25,    40,  7135,   102,\n",
       "         15677,   171,    15,    17,    25,  5773,    26,  1212,    65,  3047,\n",
       "            16,   876,    17,  2324,     9,     1,  5145,    13,   118,     1,\n",
       "          5145,    13,   118,     1,   465,     9,   300,    70,    16,   332,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [   13,    22,   124,  7959,    22,    23,    21,  5934,  1308,    88,\n",
       "            14,   153, 11285,   201,  2763,    26,   182,    17,    67, 11179,\n",
       "            40,  6090,   289,    84,    14,  2663,    15,   544,   105,  4007,\n",
       "            30,   154,  4412,    92,   340,  1151,     9,   100,    42,  2199,\n",
       "            13,    22, 18342,    14, 15771,    22,    15,    13,    22,   124,\n",
       "          7959,    22,    25,    14,  1308,    26,    42, 19015,    32,    22,\n",
       "            18,    14,   583,   254,  1308,    16,    14,   159,    17,    32,\n",
       "            25,    14,  2107,  1308,    26,    65,  5520,     9,    13,    22,\n",
       "           124,  7959,    22,  3858,    21,   394,   213,   485,   187,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 5854,    15,   811,   630,    30,   233,    48,    21,   254,  1308,\n",
       "            60,   854,    15,  1270,   510,    15,   108,    51,  4052,     9,\n",
       "          1887,  2532,    22,    38,    21,   979,   865,    20, 12360,    32,\n",
       "             9,    49,   216,    32,   253,  2276,    15,  1427, 18190,    15,\n",
       "           108,   837,     9,   242,    22,    18,    67, 24109,  1772,   212,\n",
       "           120,    17,   120,     9, 16299,    53,    17,    81,    46,  4131,\n",
       "         24109,    15,   811,    52,    28,   212,     9,    49,    67,   427,\n",
       "            48,  1308,    23,  1450,  1232,    93,    35,   436,    15,  2140,\n",
       "            31,   223,    22,    38,   510,   277,  2742,    18,    48,   231,\n",
       "            17,    14,   925,     9, 27048,  1399,  3909,  1533,    14,   407,\n",
       "           896,  1244,    19,    48, 18008,     9,   438,    22,    18,    21,\n",
       "         12238,   511,  1574,    15,   811,    31,   427,    24,   257,    33,\n",
       "           925,   266,    84,    14,   371,     9,    49,  2321,    30,  2742,\n",
       "            29,    14,  2919,    16,    14,  1308,    15,  2140,    83,    57,\n",
       "            74,   374,   100,    31,    41,  3345,    14,  1308,     9, 13349,\n",
       "            15,  1887,    46,   109,  1772,   896,    53, 12588,    18,     9,\n",
       "         10818,  1218, 13547, 10165,  4815,    19,    14,   581,   597,    15,\n",
       "           811,    25,   504,   265,    20,   170,    29,    19,    48,  1308,\n",
       "             9,   124,   925,    63,    52,   510,  7339,    15,   472,    31,\n",
       "            41,  3691,     9,  1134,   134,     9,  1565,    25,   114,    51,\n",
       "          4052,     9, 23304,    15,  1106,    55,    15, 10144,    48,  1308,\n",
       "            25,    52,    21,   779,    18,  3482,    15,   242,    25,  1772,\n",
       "           896,     9,   915,  2018,    26,   659,   177,  1867,    45,   203,\n",
       "             9, 10551,   264]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert_tokenizer = transformers.AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "albert_encoding = albert_tokenizer(\n",
    "    train_reviews[:3], padding=True, truncation=True, max_length=500,\n",
    "    add_special_tokens=False, return_tensors=\"pt\")\n",
    "albert_token_ids = albert_encoding[\"input_ids\"]\n",
    "albert_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[159, 402, 176, 246,  61, 782, 156, 737, 252,  42, 239,  51, 154, 460,\n",
       "         917,  17, 272, 156, 737, 576, 215, 976, 275,  42, 199,  44, 554,  42,\n",
       "         192, 585,  57, 160, 259, 170, 157, 143, 138, 159, 402,  11, 589, 152,\n",
       "           5, 819, 168, 230,   5, 521, 924, 981, 962, 250,  61,  10,  60, 426,\n",
       "         526, 959,  60, 138, 199, 150, 319,  15, 363, 141, 957, 694,  47, 696,\n",
       "          61, 875, 138, 960, 337, 414, 140, 157, 385, 174, 433, 161, 221, 145,\n",
       "         213,  17, 549,  15, 151,  10,  60,  55, 416, 146, 407, 144, 182, 303,\n",
       "         151, 141,  17, 138, 547, 538, 528, 768,  54, 335,  42, 203,  44, 270,\n",
       "          46, 153, 876, 141, 919, 233, 522, 172, 141, 719, 162, 807, 279,  17,\n",
       "         138,  45,  66,  55, 188, 989, 156, 378, 698, 301, 296, 689, 212, 558,\n",
       "         926, 148,  17,  44, 270,  46, 141,  47, 279, 302, 171, 152, 787,  15,\n",
       "         153, 522, 172, 766, 205, 156, 234, 677, 161, 139, 513, 146, 370, 251,\n",
       "         219, 162, 197, 162, 166,  50, 265,  47, 266, 177,  50,  10, 172, 502,\n",
       "         499, 210,  42, 163,  63, 137,  10,  60, 387,  15, 209,  50, 183, 155,\n",
       "         177,  51, 186, 774, 143, 221, 145,  10,  60, 176, 246,  61, 301, 141,\n",
       "         460,  50, 136, 355,  17, 138, 778, 141, 137, 534, 271,  43, 160, 265,\n",
       "          63, 290, 179, 157,  15, 153, 959,  60, 206, 360, 266, 148,  17,   5,\n",
       "         222, 606, 241, 246,   5, 141, 139, 145, 154,  54, 287, 160, 885, 148,\n",
       "         199,  15, 153, 141, 142, 994, 157, 182, 236, 637, 221,  47, 489, 156,\n",
       "         159, 402, 153, 718, 219, 162, 197, 162, 166,  26,  17,  23, 215, 156,\n",
       "         586,   0,   0,   0,   0],\n",
       "        [ 10, 138, 198, 289, 175,  10, 192,  42, 725, 355, 211, 311, 138, 964,\n",
       "         161, 139, 513, 493, 204, 207,  60, 182, 244, 153, 434, 679,  60, 139,\n",
       "          46, 854, 904, 733, 394, 138, 870, 415,  15, 839, 419, 433, 544,  46,\n",
       "         177, 508,  45, 142, 188,  60, 313, 576, 986,  17, 249, 206, 180, 541,\n",
       "          10, 142, 984, 138, 899, 489,  10,  15,  10, 138, 198, 289, 175,  10,\n",
       "         141, 138, 211, 182, 206, 584, 151,  10,  60, 138, 580, 321, 211, 156,\n",
       "         138, 967, 153, 151, 141, 138, 257, 684, 211, 182, 221, 281, 149,  17,\n",
       "          10, 138, 198, 289, 175,  10,  49, 377,  42, 239,  51, 154,  49, 240,\n",
       "         841,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0],\n",
       "        [289,  15, 209, 398, 177, 442, 173,  42, 321, 211,  34, 371,  15, 226,\n",
       "         370,  15, 137, 306, 583, 137, 185,  17, 291, 809,  10,  61,  42, 833,\n",
       "         384, 146, 142, 994, 151,  17,  50, 920, 151, 268,  60, 413,  15, 712,\n",
       "         399, 160,  15, 137, 727,  17, 151,  10,  60, 434, 295,  45, 543, 344,\n",
       "         735, 417, 525, 153, 525,  17,  55, 357, 440, 205, 153, 495, 382, 253,\n",
       "         303, 295,  45, 543, 344,  15, 209, 226, 152, 417,  17,  50, 434,  47,\n",
       "         203,  61, 173, 211, 192, 719,  44, 188,  57,  66, 144, 650,  15, 363,\n",
       "          50, 653,  10,  61, 370, 439,  47, 377, 173, 761, 153, 138, 331,  17,\n",
       "          51, 553,  47, 460, 183,  66, 387,  60, 138, 783, 404, 652, 137, 173,\n",
       "         137, 159, 221, 323,  17, 183,  10,  60,  42, 222, 530, 759, 477,  15,\n",
       "         209,  50,  47, 203,  61, 183, 910, 234, 331, 450, 394, 138, 919,  17,\n",
       "          50, 360, 266, 177,  47, 151, 201, 138,  61, 205, 156, 138, 211,  15,\n",
       "         363, 343, 252, 428, 403, 249,  50, 369, 190, 930, 138, 211,  17, 259,\n",
       "         244,  15, 291, 382, 253, 735, 404, 205, 849, 315,  17, 155, 174, 207,\n",
       "         838,  60, 180,  56, 142, 617,  60, 137, 138, 899, 163, 806,  15, 209,\n",
       "         141, 518, 293, 520, 146, 455, 201, 137, 173, 211,  17, 138, 331, 308,\n",
       "         226, 370, 598, 290, 541,  15, 152,  50, 369, 204, 938,  17,  56,  49,\n",
       "         371,  17, 173, 141, 299, 306, 583, 137, 185,  17, 317, 307,  15, 182,\n",
       "         250,  15, 548, 173, 211, 141, 226, 202,  66, 685, 150,  15, 151, 141,\n",
       "         735, 404,  17, 306, 838, 286, 182,  45, 900, 243,  50,  50,  50,  29,\n",
       "          22,  17,  24,  18,  24]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer = transformers.PreTrainedTokenizerFast(\n",
    "    tokenizer_object=bpe_tokenizer)\n",
    "hf_encodings = hf_tokenizer(train_reviews[:3], padding=True, truncation=True,\n",
    "                            max_length=500, return_tensors=\"pt\")\n",
    "hf_encodings[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training a Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer=bert_tokenizer):\n",
    "    reviews = [review[\"text\"] for review in batch]\n",
    "    labels = [[review[\"label\"]] for review in batch]\n",
    "    encodings = tokenizer(reviews, padding=True, truncation=True,\n",
    "                          max_length=200, return_tensors=\"pt\")\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    return encodings, labels\n",
    "\n",
    "batch_size = 256\n",
    "imdb_train_loader = DataLoader(imdb_train_set, batch_size=batch_size,\n",
    "                               collate_fn=collate_fn, shuffle=True)\n",
    "imdb_valid_loader = DataLoader(imdb_valid_set, batch_size=batch_size,\n",
    "                               collate_fn=collate_fn)\n",
    "imdb_test_loader = DataLoader(imdb_test_set, batch_size=batch_size,\n",
    "                              collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layers=2, embed_dim=128, hidden_dim=64,\n",
    "                 pad_id=0, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim,\n",
    "                                  padding_idx=pad_id)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "        self.output = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, encodings):\n",
    "        embeddings = self.embed(encodings[\"input_ids\"])\n",
    "        _outputs, hidden_states = self.gru(embeddings)\n",
    "        return self.output(hidden_states[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([5, 1, 6, 2, 7, 8]), batch_sizes=tensor([2, 2, 1, 1]), sorted_indices=tensor([1, 0]), unsorted_indices=tensor([1, 0]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "sequences = torch.tensor([[1, 2, 0, 0], [5, 6, 7, 8]])\n",
    "packed = pack_padded_sequence(sequences, lengths=(2, 4),\n",
    "                              enforce_sorted=False, batch_first=True)\n",
    "packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 0, 0],\n",
       "         [5, 6, 7, 8]]),\n",
       " tensor([2, 4]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded, lengths = pad_packed_sequence(packed, batch_first=True)\n",
    "padded, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModelPackedSeq(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layers=2, embed_dim=128,\n",
    "                 hidden_dim=64, pad_id=0, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim,\n",
    "                                  padding_idx=pad_id)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "        self.output = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, encodings):\n",
    "        embeddings = self.embed(encodings[\"input_ids\"])\n",
    "        lengths = encodings[\"attention_mask\"].sum(dim=1)                      # <= line added\n",
    "        packed = pack_padded_sequence(embeddings, lengths=lengths.cpu(),      # <= line added\n",
    "                                      batch_first=True, enforce_sorted=False) # <= line added\n",
    "        _outputs, hidden_states = self.gru(packed)                            # <= line changed\n",
    "        return self.output(hidden_states[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10,                      train loss: 0.6795, train metric: 57.05%, valid metric: 55.84%\n",
      "Epoch 2/10,                      train loss: 0.6118, train metric: 67.74%, valid metric: 59.58%\n",
      "Epoch 3/10,                      train loss: 0.4613, train metric: 78.65%, valid metric: 80.68%\n",
      "Epoch 4/10,                      train loss: 0.3391, train metric: 85.62%, valid metric: 83.52%\n",
      "Epoch 5/10,                      train loss: 0.2547, train metric: 89.93%, valid metric: 84.66%\n",
      "Epoch 6/10,                      train loss: 0.2894, train metric: 88.05%, valid metric: 81.98%\n",
      "Epoch 7/10,                      train loss: 0.1837, train metric: 93.08%, valid metric: 84.26%\n",
      "Epoch 8/10,                      train loss: 0.1181, train metric: 96.04%, valid metric: 82.80%\n",
      "Epoch 9/10,                      train loss: 0.0648, train metric: 98.22%, valid metric: 83.44%\n",
      "Epoch 10/10,                      train loss: 0.0488, train metric: 98.80%, valid metric: 83.30%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "vocab_size = bert_tokenizer.vocab_size\n",
    "imdb_model_ps = SentimentAnalysisModelPackedSeq(vocab_size).to(device)\n",
    "\n",
    "n_epochs = 10\n",
    "xentropy = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.NAdam(imdb_model_ps.parameters())\n",
    "accuracy = torchmetrics.Accuracy(task=\"binary\").to(device)\n",
    "\n",
    "history = train(imdb_model_ps, optimizer, xentropy, accuracy,\n",
    "                imdb_train_loader, imdb_valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModelBidi(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layers=2, embed_dim=128,\n",
    "                 hidden_dim=64, pad_id=0, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim,\n",
    "                                  padding_idx=pad_id)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers,\n",
    "                          batch_first=True, dropout=dropout, bidirectional=True)  # <= line changed\n",
    "        self.output = nn.Linear(2 * hidden_dim, 1)                               # <= line changed\n",
    "\n",
    "    def forward(self, encodings):\n",
    "        embeddings = self.embed(encodings[\"input_ids\"])\n",
    "        lengths = encodings[\"attention_mask\"].sum(dim=1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths=lengths.cpu(),\n",
    "                                      batch_first=True, enforce_sorted=False)\n",
    "        _outputs, hidden_states = self.gru(packed)\n",
    "        n_dims = self.output.in_features                                          # <= line added\n",
    "        top_states = hidden_states[-2:].permute(1, 0, 2).reshape(-1, n_dims)      # <= line added\n",
    "        return self.output(top_states)                                            # <= line changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10,                      train loss: 0.6514, train metric: 60.40%, valid metric: 56.66%\n",
      "Epoch 2/10,                      train loss: 0.5051, train metric: 75.31%, valid metric: 74.18%\n",
      "Epoch 3/10,                      train loss: 0.3939, train metric: 82.45%, valid metric: 82.12%\n",
      "Epoch 4/10,                      train loss: 0.2917, train metric: 87.91%, valid metric: 80.72%\n",
      "Epoch 5/10,                      train loss: 0.2114, train metric: 91.63%, valid metric: 83.94%\n",
      "Epoch 6/10,                      train loss: 0.1501, train metric: 94.41%, valid metric: 83.98%\n",
      "Epoch 7/10,                      train loss: 0.0854, train metric: 97.29%, valid metric: 81.14%\n",
      "Epoch 8/10,                      train loss: 0.0834, train metric: 97.10%, valid metric: 84.30%\n",
      "Epoch 9/10,                      train loss: 0.0246, train metric: 99.42%, valid metric: 83.80%\n",
      "Epoch 10/10,                      train loss: 0.0109, train metric: 99.80%, valid metric: 83.56%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "vocab_size = bert_tokenizer.vocab_size\n",
    "imdb_model_bidi = SentimentAnalysisModelBidi(vocab_size).to(device)\n",
    "\n",
    "n_epochs = 10\n",
    "xentropy = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.NAdam(imdb_model_bidi.parameters())\n",
    "accuracy = torchmetrics.Accuracy(task=\"binary\").to(device)\n",
    "\n",
    "history = train(imdb_model_bidi, optimizer, xentropy, accuracy, imdb_train_loader,\n",
    "                imdb_valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, let's clean up the GPU RAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "Out.clear()  # clear Jupyter's `Out` variable which saves all the cell outputs\n",
    "del_vars([\"albert_token_ids\", \"attention_mask\", \"bpe_batch_ids\", \"encoded_text\",\n",
    "          \"lengths\", \"optimizer\", \"padded\", \"probs\", \"samples\", \"sequences\",\n",
    "          \"x\", \"xentropy\", \"y\", \"Y_logits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Embeddings and Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babad2d6e62848be81f7f28e68729084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model.embeddings.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModelPreEmbeds(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings, n_layers=2, hidden_dim=64,\n",
    "                 dropout=0.2):\n",
    "        super().__init__()\n",
    "        weights = pretrained_embeddings.weight.data\n",
    "        self.embed = nn.Embedding.from_pretrained(weights, freeze=True)\n",
    "        embed_dim = weights.shape[-1]\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers,\n",
    "                          batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.output = nn.Linear(2 * hidden_dim, 1)\n",
    "\n",
    "    def forward(self, encodings):\n",
    "        embeddings = self.embed(encodings[\"input_ids\"])\n",
    "        lengths = encodings[\"attention_mask\"].sum(dim=1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths=lengths.cpu(),\n",
    "                                      batch_first=True, enforce_sorted=False)\n",
    "        _outputs, hidden_states = self.gru(packed)\n",
    "        n_dims = self.output.in_features\n",
    "        top_states = hidden_states[-2:].permute(1, 0, 2).reshape(-1, n_dims)\n",
    "        return self.output(top_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10,                      train loss: 0.6896, train metric: 54.61%, valid metric: 56.50%\n",
      "Epoch 2/10,                      train loss: 0.6583, train metric: 63.41%, valid metric: 69.16%\n",
      "Epoch 3/10,                      train loss: 0.5599, train metric: 71.92%, valid metric: 72.56%\n",
      "Epoch 4/10,                      train loss: 0.4806, train metric: 77.27%, valid metric: 64.06%\n",
      "Epoch 5/10,                      train loss: 0.4019, train metric: 81.57%, valid metric: 82.24%\n",
      "Epoch 6/10,                      train loss: 0.3624, train metric: 83.79%, valid metric: 83.94%\n",
      "Epoch 7/10,                      train loss: 0.3333, train metric: 85.51%, valid metric: 79.96%\n",
      "Epoch 8/10,                      train loss: 0.3118, train metric: 86.58%, valid metric: 80.74%\n",
      "Epoch 9/10,                      train loss: 0.2938, train metric: 87.45%, valid metric: 63.74%\n",
      "Epoch 10/10,                      train loss: 0.2664, train metric: 88.83%, valid metric: 79.90%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "imdb_model_bert_embeds = SentimentAnalysisModelPreEmbeds(\n",
    "    bert_model.embeddings.word_embeddings).to(device)\n",
    "\n",
    "n_epochs = 10\n",
    "xentropy = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.NAdam(imdb_model_bert_embeds.parameters())\n",
    "accuracy = torchmetrics.Accuracy(task=\"binary\").to(device)\n",
    "\n",
    "history = train(imdb_model_bert_embeds, optimizer, xentropy, accuracy,\n",
    "                imdb_train_loader, imdb_valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 200, 768])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_encoding = bert_tokenizer(train_reviews[:3], padding=True,\n",
    "                               max_length=200, truncation=True,\n",
    "                               return_tensors=\"pt\")\n",
    "bert_output = bert_model(**bert_encoding)\n",
    "bert_output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_output.pooler_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's free some GPU RAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_vars([\"bert_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModelBert(nn.Module):\n",
    "    def __init__(self, n_layers=2, hidden_dim=64, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.bert = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        embed_dim = self.bert.config.hidden_size\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "        self.output = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, encodings):\n",
    "        contextualized_embeddings = self.bert(**encodings).last_hidden_state\n",
    "        lengths = encodings[\"attention_mask\"].sum(dim=1)\n",
    "        packed = pack_padded_sequence(contextualized_embeddings, lengths=lengths.cpu(),\n",
    "                                      batch_first=True, enforce_sorted=False)\n",
    "        _outputs, hidden_states = self.gru(packed)\n",
    "        return self.output(hidden_states[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4,                      train loss: 0.4879, train metric: 75.29%, valid metric: 87.22%\n",
      "Epoch 2/4,                      train loss: 0.3072, train metric: 86.91%, valid metric: 88.26%\n",
      "Epoch 3/4,                      train loss: 0.2779, train metric: 88.44%, valid metric: 88.20%\n",
      "Epoch 4/4,                      train loss: 0.2575, train metric: 89.22%, valid metric: 88.00%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "imdb_model_bert = SentimentAnalysisModelBert().to(device)\n",
    "imdb_model_bert.bert.requires_grad_(False)\n",
    "\n",
    "n_epochs = 4\n",
    "xentropy = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.NAdam(imdb_model_bert.parameters())\n",
    "accuracy = torchmetrics.Accuracy(task=\"binary\").to(device)\n",
    "\n",
    "history = train(imdb_model_bert, optimizer, xentropy, accuracy,\n",
    "                imdb_train_loader, imdb_valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's free some GPU RAM again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_vars([\"imdb_model_bert\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModelBert2(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.bert = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.output = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, encodings):\n",
    "        bert_output = self.bert(**encodings)\n",
    "        return self.output(bert_output.last_hidden_state[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModelBert3(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.bert = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.output = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, encodings):\n",
    "        bert_output = self.bert(**encodings)\n",
    "        return self.output(bert_output.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5,                      train loss: 0.6602, train metric: 61.26%, valid metric: 71.66%\n",
      "Epoch 2/5,                      train loss: 0.6033, train metric: 69.37%, valid metric: 66.74%\n",
      "Epoch 3/5,                      train loss: 0.5716, train metric: 72.45%, valid metric: 75.18%\n",
      "Epoch 4/5,                      train loss: 0.5535, train metric: 73.12%, valid metric: 73.04%\n",
      "Epoch 5/5,                      train loss: 0.5343, train metric: 74.74%, valid metric: 59.80%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "imdb_model_bert3 = SentimentAnalysisModelBert3().to(device)\n",
    "imdb_model_bert3.bert.requires_grad_(False)\n",
    "\n",
    "n_epochs = 5\n",
    "xentropy = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.NAdam(imdb_model_bert3.parameters())\n",
    "accuracy = torchmetrics.Accuracy(task=\"binary\").to(device)\n",
    "\n",
    "history = train(imdb_model_bert3, optimizer, xentropy, accuracy,\n",
    "                imdb_train_loader, imdb_valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5,                      train loss: 0.8155, train metric: 71.27%, valid metric: 81.72%\n",
      "Epoch 2/5,                      train loss: 0.4582, train metric: 78.73%, valid metric: 82.40%\n",
      "Epoch 3/5,                      train loss: 0.4408, train metric: 79.41%, valid metric: 81.02%\n",
      "Epoch 4/5,                      train loss: 0.4216, train metric: 80.86%, valid metric: 81.26%\n",
      "Epoch 5/5,                      train loss: 0.4172, train metric: 80.87%, valid metric: 82.20%\n"
     ]
    }
   ],
   "source": [
    "imdb_model_bert3.bert.pooler.requires_grad_(True)\n",
    "\n",
    "history = train(imdb_model_bert3, optimizer, xentropy, accuracy,\n",
    "                imdb_train_loader, imdb_valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's free some GPU RAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_vars([\"imdb_model_bert3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-Specific Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "torch.manual_seed(42)\n",
    "bert_for_binary_clf = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2, dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0120,  0.6304]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "encoding = bert_tokenizer([\"This was a great movie!\"])\n",
    "with torch.no_grad():\n",
    "    output = bert_for_binary_clf(\n",
    "        input_ids=torch.tensor(encoding[\"input_ids\"], device=device),\n",
    "        attention_mask=torch.tensor(encoding[\"attention_mask\"], device=device))\n",
    "\n",
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3447, 0.6553]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "torch.softmax(output.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4226, device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = bert_for_binary_clf(\n",
    "        input_ids=torch.tensor(encoding[\"input_ids\"], device=device),\n",
    "        attention_mask=torch.tensor(encoding[\"attention_mask\"], device=device),\n",
    "        labels=torch.tensor([1], device=device))\n",
    "\n",
    "output.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every model from the Transformers library has a `config` attribute that contains the model's configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"dtype\": \"float32\",\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"problem_type\": \"single_label_classification\",\n",
       "  \"transformers_version\": \"4.56.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_for_binary_clf.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Trainer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use our own training function to train this model, or we can use the Trainer API instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Trainer API needs datasets that are already tokenized, so let's prepare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d5faea1dc44154a904deff20d4360c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7f96c9e98a484f867aabb7d6d6ecd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02e80a808ee4cd9958874dc6b6debba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_batch(batch):\n",
    "    return bert_tokenizer(batch[\"text\"], truncation=True, max_length=200)\n",
    "\n",
    "tok_imdb_train_set = imdb_train_set.map(tokenize_batch, batched=True)\n",
    "tok_imdb_valid_set = imdb_valid_set.map(tokenize_batch, batched=True)\n",
    "tok_imdb_test_set = imdb_test_set.map(tokenize_batch, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Trainer API doesn't support streaming metrics so we need to write our own evaluation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(pred):\n",
    "    return {\"accuracy\": (pred.label_ids == pred.predictions.argmax(-1)).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"my_imdb_model\", num_train_epochs=2,\n",
    "    per_device_train_batch_size=128, per_device_eval_batch_size=128,\n",
    "    eval_strategy=\"epoch\", logging_strategy=\"epoch\", save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True, metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='314' max='314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [314/314 05:06, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.310200</td>\n",
       "      <td>0.237499</td>\n",
       "      <td>0.903400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.239066</td>\n",
       "      <td>0.908200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    bert_for_binary_clf, train_args, train_dataset=tok_imdb_train_set,\n",
    "    eval_dataset=tok_imdb_valid_set, compute_metrics=compute_accuracy,\n",
    "    data_collator=DataCollatorWithPadding(bert_tokenizer))\n",
    "train_output = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's free some GPU RAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_vars([\"bert_for_binary_clf\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c948bf4ee244ada03ef159fd10698e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3f7a4e305d4bd9a5a1d80633740d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbcf1b033ab94ea08ce895b12be982f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01321633f38c4774ae7db11ab62d7c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9996108412742615},\n",
       " {'label': 'POSITIVE', 'score': 0.9998623132705688},\n",
       " {'label': 'NEGATIVE', 'score': 0.9943684935569763},\n",
       " {'label': 'POSITIVE', 'score': 0.997913658618927},\n",
       " {'label': 'POSITIVE', 'score': 0.999544084072113},\n",
       " {'label': 'NEGATIVE', 'score': 0.9845332503318787},\n",
       " {'label': 'POSITIVE', 'score': 0.9859278202056885},\n",
       " {'label': 'POSITIVE', 'score': 0.9993758797645569},\n",
       " {'label': 'POSITIVE', 'score': 0.9978922009468079},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997020363807678}]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "classifier_imdb = pipeline(\"sentiment-analysis\", model=model_name,\n",
    "                           truncation=True, max_length=512)\n",
    "classifier_imdb(train_reviews[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/20"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8820, device='cuda:0')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = torchmetrics.Accuracy(task=\"binary\").to(device)\n",
    "with torch.no_grad():\n",
    "    text_imdb_valid_loader = DataLoader(imdb_valid_set, batch_size=256)\n",
    "    for index, batch in enumerate(text_imdb_valid_loader):\n",
    "        y_pred = classifier_imdb(batch[\"text\"], truncation=True)\n",
    "        y_pred = torch.tensor([pred[\"label\"] == \"POSITIVE\" for pred in y_pred], dtype=int)\n",
    "        accuracy.update(y_pred, batch[\"label\"])\n",
    "        print(f\"\\r{index + 1}/{len(text_imdb_valid_loader)}\", end=\"\")\n",
    "\n",
    "accuracy.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be very biased. For example, it may like or dislike some countries depending on the data it was trained on, and how it is used, so use it with care:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Iraq', {'label': 'NEGATIVE', 'score': 0.9706069231033325}),\n",
       " ('Thailand', {'label': 'POSITIVE', 'score': 0.9903932213783264}),\n",
       " ('the USA', {'label': 'POSITIVE', 'score': 0.9642282128334045}),\n",
       " ('Vietnam', {'label': 'NEGATIVE', 'score': 0.9747399091720581})]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code â€“ shows that binary classification can amplify model bias\n",
    "countries = [\"Iraq\", \"Thailand\", \"the USA\", \"Vietnam\"]\n",
    "texts = [f\"I am from {country}\" for country in countries]\n",
    "list(zip(countries, classifier_imdb(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede5def1e2694449947d5119f35cfd5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3333189de7434196a1a1cc717e8b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0745ce68ea844aeae8a44f873fb5399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5020e8448874551ab52528398c04aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82cfc59d9c0438e9c3d72570a437196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a716432f2cdb44bb87efae7efec2a7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Iraq', {'label': 'neutral', 'score': 0.8353288769721985}),\n",
       " ('Thailand', {'label': 'neutral', 'score': 0.8824347853660583}),\n",
       " ('the USA', {'label': 'neutral', 'score': 0.8349122405052185}),\n",
       " ('Vietnam', {'label': 'neutral', 'score': 0.8436853885650635})]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code â€“ using a model with a neutral class solves this bias issue\n",
    "# note: the warning is normal: this model's pooler will not be used for\n",
    "# classification, so its weights are downloaded but not used.\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "classifier_imdb_with_neutral = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "list(zip(countries, classifier_imdb_with_neutral(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa97d6730bc413581bab0b79f7a0f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/729 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38b6a9591354fa989f0efbe78b371b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5583fcfce6ca4d7d8aa3c3c63a60549a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/58.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f892819abbf4c968a8f24d0afacaaa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b8cae01ba6416799dacd42680da4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1622bd752f4502a5cf532ad1c60909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'contradiction', 'score': 0.9717152714729309},\n",
       " {'label': 'entailment', 'score': 0.9119168519973755},\n",
       " {'label': 'neutral', 'score': 0.9509281516075134}]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"huggingface/distilbert-base-uncased-finetuned-mnli\"\n",
    "classifier_mnli = pipeline(\"text-classification\", model=model_name)\n",
    "classifier_mnli([\n",
    "    \"She loves me. [SEP] She loves me not. [SEP]\",\n",
    "    \"Alice just woke up. [SEP] Alice is awake. [SEP]\",\n",
    "    \"I like dogs. [SEP] Everyone likes dogs. [SEP]\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's free some GPU RAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "Out.clear()\n",
    "del_vars([\"classifier_imdb\", \"classifier_mnli\", \"classifier_imdb_with_neutral\",\n",
    "          \"trainer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Encoderâ€“Decoder Network for Neural Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to start the notebook here, please run the cells in the Setup\n",
    "# section at the start of the notebook, then come back to this cell\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a68d5de3dd4df7bdce184863274ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219df15ca8d3470192c648f74d106bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eng-spa/validation-00000-of-00001.parque(â€¦):   0%|          | 0.00/7.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f10e4765a140969f3c7a90fbaffce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eng-spa/test-00000-of-00001.parquet:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3feb99a2b09d42a39d4cb0e44edc9526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/197299 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e2f23c086642d4be3929866cf177b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/24514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nmt_original_valid_set, nmt_test_set = load_dataset(\n",
    "    path=\"ageron/tatoeba_mt_train\", name=\"eng-spa\",\n",
    "    split=[\"validation\", \"test\"])\n",
    "split = nmt_original_valid_set.train_test_split(train_size=0.8, seed=42)\n",
    "nmt_train_set, nmt_valid_set = split[\"train\"], split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_text': 'Tom tried to break up the fight.',\n",
       " 'target_text': 'Tom tratÃ³ de disolver la pelea.',\n",
       " 'source_lang': 'eng',\n",
       " 'target_lang': 'spa'}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt_train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eng_spa():  # a generator function to iterate over all training text\n",
    "    for pair in nmt_train_set:\n",
    "        yield pair[\"source_text\"]\n",
    "        yield pair[\"target_text\"]\n",
    "\n",
    "max_length = 256\n",
    "vocab_size = 10_000\n",
    "\n",
    "nmt_tokenizer_model = tokenizers.models.BPE(unk_token=\"<unk>\")\n",
    "nmt_tokenizer = tokenizers.Tokenizer(nmt_tokenizer_model)\n",
    "nmt_tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\")\n",
    "nmt_tokenizer.enable_truncation(max_length=max_length)\n",
    "nmt_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
    "nmt_tokenizer_trainer = tokenizers.trainers.BpeTrainer(\n",
    "    vocab_size=vocab_size, special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"])\n",
    "nmt_tokenizer.train_from_iterator(train_eng_spa(), nmt_tokenizer_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43, 401, 4381]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt_tokenizer.encode(\"I like soccer\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 396, 582, 219, 3356]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt_tokenizer.encode(\"<s> Me gusta el fÃºtbol\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "fields = [\"src_token_ids\", \"src_mask\", \"tgt_token_ids\", \"tgt_mask\"]\n",
    "class NmtPair(namedtuple(\"NmtPairBase\", fields)):\n",
    "    def to(self, device):\n",
    "        return NmtPair(self.src_token_ids.to(device), self.src_mask.to(device),\n",
    "                       self.tgt_token_ids.to(device), self.tgt_mask.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmt_collate_fn(batch):\n",
    "    src_texts = [pair['source_text'] for pair in batch]\n",
    "    tgt_texts = [f\"<s> {pair['target_text']} </s>\" for pair in batch]\n",
    "    src_encodings = nmt_tokenizer.encode_batch(src_texts)\n",
    "    tgt_encodings = nmt_tokenizer.encode_batch(tgt_texts)\n",
    "    src_token_ids = torch.tensor([enc.ids for enc in src_encodings])\n",
    "    tgt_token_ids = torch.tensor([enc.ids for enc in tgt_encodings])\n",
    "    src_mask = torch.tensor([enc.attention_mask for enc in src_encodings])\n",
    "    tgt_mask = torch.tensor([enc.attention_mask for enc in tgt_encodings])\n",
    "    inputs = NmtPair(src_token_ids, src_mask,\n",
    "                     tgt_token_ids[:, :-1], tgt_mask[:, :-1])\n",
    "    labels = tgt_token_ids[:, 1:]\n",
    "    return inputs, labels\n",
    "\n",
    "batch_size = 32\n",
    "nmt_train_loader = DataLoader(nmt_train_set, batch_size=batch_size,\n",
    "                              collate_fn=nmt_collate_fn, shuffle=True)\n",
    "nmt_valid_loader = DataLoader(nmt_valid_set, batch_size=batch_size,\n",
    "                              collate_fn=nmt_collate_fn)\n",
    "nmt_test_loader = DataLoader(nmt_test_set, batch_size=batch_size,\n",
    "                             collate_fn=nmt_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NmtModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, pad_id=0, hidden_dim=512,\n",
    "                 n_layers=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
    "        self.encoder = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers,\n",
    "                              batch_first=True)\n",
    "        self.decoder = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers,\n",
    "                              batch_first=True)\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, pair):\n",
    "        src_embeddings = self.embed(pair.src_token_ids)\n",
    "        tgt_embeddings = self.embed(pair.tgt_token_ids)\n",
    "        src_lengths = pair.src_mask.sum(dim=1)\n",
    "        src_packed = pack_padded_sequence(\n",
    "            src_embeddings, lengths=src_lengths.cpu(),\n",
    "            batch_first=True, enforce_sorted=False)\n",
    "        _, hidden_states = self.encoder(src_packed)\n",
    "        outputs, _ = self.decoder(tgt_embeddings, hidden_states)\n",
    "        return self.output(outputs).permute(0, 2, 1)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "vocab_size = nmt_tokenizer.get_vocab_size()\n",
    "nmt_model = NmtModel(vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10,                      train loss: 3.1338, train metric: 17.29%, valid metric: 20.30%\n",
      "Epoch 2/10,                      train loss: 2.0359, train metric: 21.86%, valid metric: 21.31%\n",
      "Epoch 3/10,                      train loss: 1.7177, train metric: 23.36%, valid metric: 21.52%\n",
      "Epoch 4/10,                      train loss: 1.5585, train metric: 24.21%, valid metric: 21.51%\n",
      "Epoch 5/10,                      train loss: 1.4643, train metric: 24.59%, valid metric: 21.42%\n",
      "Epoch 6/10,                      train loss: 1.4143, train metric: 24.83%, valid metric: 21.32%\n",
      "Epoch 7/10,                      train loss: 1.0877, train metric: 26.90%, valid metric: 22.21%\n",
      "Epoch 8/10,                      train loss: 0.8589, train metric: 28.78%, valid metric: 22.25%\n",
      "Epoch 9/10,                      train loss: 0.7411, train metric: 29.56%, valid metric: 22.20%\n",
      "Epoch 10/10,                      train loss: 0.6579, train metric: 30.26%, valid metric: 22.15%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "xentropy = nn.CrossEntropyLoss(ignore_index=0)  # ignore <pad> tokens\n",
    "optimizer = torch.optim.NAdam(nmt_model.parameters(), lr=0.001)\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocab_size)\n",
    "accuracy = accuracy.to(device)\n",
    "\n",
    "history = train(nmt_model, optimizer, xentropy, accuracy,\n",
    "                nmt_train_loader, nmt_valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(nmt_model.state_dict(), \"my_nmt_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, src_text, max_length=20, pad_id=0, eos_id=3):\n",
    "    tgt_text = \"\"\n",
    "    token_ids = []\n",
    "    for index in range(max_length):\n",
    "        batch, _ = nmt_collate_fn([{\"source_text\": src_text,\n",
    "                                    \"target_text\": tgt_text}])\n",
    "        with torch.no_grad():\n",
    "            Y_logits = model(batch.to(device))\n",
    "            Y_token_ids = Y_logits.argmax(dim=1)  # find the best token IDs\n",
    "            next_token_id = Y_token_ids[0, index]  # take the last token ID\n",
    "\n",
    "        next_token = nmt_tokenizer.id_to_token(next_token_id)\n",
    "        tgt_text += \" \" + next_token\n",
    "        if next_token_id == eos_id:\n",
    "            break\n",
    "    return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Me gusta el fÃºtbol . </s>'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt_model.eval()\n",
    "translate(nmt_model, \"I like soccer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Me gusta jugar con mis amigos . </s>'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longer_text = \"I like to play soccer with my friends.\"\n",
    "translate(nmt_model, longer_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very basic implementation of beam search. I tried to make it readable and understandable, but it's definitely not optimized for speed! The function first uses the model to find the top _k_ words to start the translations (where _k_ is the beam width). For each of the top _k_ translations, it evaluates the conditional probabilities of all possible words it could add to that translation. These extended translations and their probabilities are added to the list of candidates. Once we've gone through all top _k_ translations and all words that could complete them, we keep only the top _k_ candidates with the highest probability, and we iterate over and over until they all finish with an EOS token. The top translation is then returned (after removing its EOS token).\n",
    "\n",
    "* Note: If p(S) is the probability of sentence S, and p(W|S) is the conditional probability of the word W given that the translation starts with S, then the probability of the sentence S' = concat(S, W) is p(S') = p(S) * p(W|S). As we add more words, the probability gets smaller and smaller. To avoid the risk of it getting too small, which could cause floating point precision errors, the function keeps track of log probabilities instead of probabilities: recall that log(a\\*b) = log(a) + log(b), therefore log(p(S')) = log(p(S)) + log(p(W|S))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, src_text, beam_width=3, max_length=20,\n",
    "                verbose=False, length_penalty=0.6):\n",
    "    top_translations = [(torch.tensor(0.), \"\")]\n",
    "    for index in range(max_length):\n",
    "        if verbose:\n",
    "            print(f\"Top {beam_width} translations so far:\")\n",
    "            for log_proba, tgt_text in top_translations:\n",
    "                print(f\"    {log_proba.item():.3f} â€“ {tgt_text}\")\n",
    "\n",
    "        candidates = []\n",
    "        for log_proba, tgt_text in top_translations:\n",
    "            if tgt_text.endswith(\" </s>\"):\n",
    "                candidates.append((log_proba, tgt_text))\n",
    "                continue  # don't add tokens after EOS token\n",
    "            batch, _ = nmt_collate_fn([{\"source_text\": src_text,\n",
    "                                        \"target_text\": tgt_text}])\n",
    "            with torch.no_grad():\n",
    "                Y_logits = model(batch.to(device))\n",
    "                Y_log_proba = F.log_softmax(Y_logits, dim=1)\n",
    "                Y_top_log_probas = torch.topk(Y_log_proba, k=beam_width, dim=1)\n",
    "\n",
    "            for beam_index in range(beam_width):\n",
    "                next_token_log_proba = Y_top_log_probas.values[0, beam_index, index]\n",
    "                next_token_id = Y_top_log_probas.indices[0, beam_index, index]\n",
    "                next_token = nmt_tokenizer.id_to_token(next_token_id)\n",
    "                next_tgt_text = tgt_text + \" \" + next_token\n",
    "                candidates.append((log_proba + next_token_log_proba, next_tgt_text))\n",
    "\n",
    "        def length_penalized_score(candidate, alpha=length_penalty):\n",
    "            log_proba, text = candidate\n",
    "            length = len(text.split())\n",
    "            penalty = ((5 + length) ** alpha) / (6 ** alpha)\n",
    "            return log_proba / penalty\n",
    "\n",
    "        top_translations = sorted(candidates,\n",
    "                                  key=length_penalized_score,\n",
    "                                  reverse=True)[:beam_width]\n",
    "\n",
    "    return top_translations[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Me gusta jugar al fÃºtbol con mis amigos . </s>'\n"
     ]
    }
   ],
   "source": [
    "beam_search(nmt_model, longer_text, beam_width=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' Me gusta jugar con jugar con los jug adores de la playa . </s>'\n"
     ]
    }
   ],
   "source": [
    "longest_text = \"I like to play soccer with my friends at the beach.\"\n",
    "beam_search(nmt_model, longest_text, beam_width=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's free some GPU RAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_vars([\"nmt_model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement Luong attention (a.k.a. dot-product attention):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value):  # note: dq == dk and Lk == Lv\n",
    "    scores = query @ key.transpose(1, 2)  # [B,Lq,dq] @ [B,dk,Lk] = [B, Lq, Lk]\n",
    "    weights = torch.softmax(scores, dim=-1)  # [B, Lq, Lk]\n",
    "    return weights @ value  # [B, Lq, Lk] @ [B, Lv, dv] = [B, Lq, dv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `B` = batch size\n",
    "* `Lq` = max query length in this batch\n",
    "* `Lk` = max key length in this batch = `Lv` = max value length in this batch\n",
    "* `dq` = query embedding size = `dk` = key embedding size\n",
    "* `dv` = value embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NmtModelWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, pad_id=0, hidden_dim=512,\n",
    "                 n_layers=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
    "        self.encoder = nn.GRU(\n",
    "            embed_dim, hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.decoder = nn.GRU(\n",
    "            embed_dim, hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.output = nn.Linear(2 * hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, pair):\n",
    "        src_embeddings = self.embed(pair.src_token_ids)  # same as earlier\n",
    "        tgt_embeddings = self.embed(pair.tgt_token_ids)  # same\n",
    "        src_lengths = pair.src_mask.sum(dim=1)  # same\n",
    "        src_packed = pack_padded_sequence(\n",
    "            src_embeddings, lengths=src_lengths.cpu(),\n",
    "            batch_first=True, enforce_sorted=False)  # same\n",
    "        encoder_outputs_packed, hidden_states = self.encoder(src_packed)\n",
    "        decoder_outputs, _ = self.decoder(tgt_embeddings, hidden_states)  # same\n",
    "        encoder_outputs, _ = pad_packed_sequence(encoder_outputs_packed,\n",
    "                                                 batch_first=True)\n",
    "        attn_output = attention(query=decoder_outputs,\n",
    "                                key=encoder_outputs, value=encoder_outputs)\n",
    "        combined_output = torch.cat((attn_output, decoder_outputs), dim=-1)\n",
    "        return self.output(combined_output).permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10,                      train loss: 3.0072, train metric: 17.94%, valid metric: 20.44%\n",
      "Epoch 2/10,                      train loss: 2.1242, train metric: 21.48%, valid metric: 21.15%\n",
      "Epoch 3/10,                      train loss: 1.9239, train metric: 22.38%, valid metric: 21.29%\n",
      "Epoch 4/10,                      train loss: 1.8401, train metric: 22.76%, valid metric: 21.25%\n",
      "Epoch 5/10,                      train loss: 1.7882, train metric: 23.05%, valid metric: 21.25%\n",
      "Epoch 6/10,                      train loss: 1.7606, train metric: 23.14%, valid metric: 21.23%\n",
      "Epoch 7/10,                      train loss: 1.4143, train metric: 25.02%, valid metric: 22.39%\n",
      "Epoch 8/10,                      train loss: 1.1987, train metric: 26.35%, valid metric: 22.59%\n",
      "Epoch 9/10,                      train loss: 1.0960, train metric: 26.93%, valid metric: 22.71%\n",
      "Epoch 10/10,                      train loss: 1.0242, train metric: 27.44%, valid metric: 22.73%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "nmt_attn_model = NmtModelWithAttention(vocab_size).to(device)\n",
    "\n",
    "n_epochs = 10\n",
    "xentropy = nn.CrossEntropyLoss(ignore_index=0)  # ignore <pad> tokens\n",
    "optimizer = torch.optim.NAdam(nmt_attn_model.parameters(), lr=0.001)\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocab_size)\n",
    "accuracy = accuracy.to(device)\n",
    "\n",
    "history = train(nmt_attn_model, optimizer, xentropy, accuracy,\n",
    "                nmt_train_loader, nmt_valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(nmt_attn_model.state_dict(), \"my_nmt_attn_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Me gusta jugar al fÃºtbol con mis amigos . </s>'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(nmt_attn_model, longer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' Me gusta jugar fu tbol con mis amigos en la playa . </s>'\n"
     ]
    }
   ],
   "source": [
    "translate(nmt_attn_model, longest_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's free some GPU RAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_vars([\"nmt_attn_model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Material â€“ Exploring Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load BERT's pretrained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96d7bde68d34b8c842520dd96f3b592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "embedding_matrix = model.get_input_embeddings().weight.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a little helper function to get a given token's embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embedding(token):\n",
    "    token_id = tokenizer.vocab[token]\n",
    "    return embedding_matrix[token_id]\n",
    "\n",
    "get_embedding(\"hello\").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes three tokens, computes `E(token2) - E(token1) + E(token3)` (where E(token) is the token's embedding) and finds the most similar token embeddings, using the _cosine similarity_. The cosine similarity between two vectors is the cosine of the angle between the vectors, so its value ranges from â€“1 (completely opposite) to +1 (perfectly aligned). It returns a list of (similarity, token) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def find_closest_tokens(token1, token2, token3, top_n=5):\n",
    "    E = get_embedding\n",
    "    result = E(token2) - E(token1) + E(token3)\n",
    "    similarities = F.cosine_similarity(result, embedding_matrix)\n",
    "    top_k = torch.topk(similarities, k=top_n)\n",
    "    return [(sim.item(), tokenizer.decode(idx.item()))\n",
    "            for sim, idx in zip(top_k.values, top_k.indices)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king is to queen as man is to: man (0.7) woman (0.6) queen (0.5) girl (0.5) lady (0.5) \n",
      "man is to woman as nephew is to: nephew (0.8) niece (0.8) granddaughter (0.7) grandson (0.7) daughters (0.6) \n",
      "father is to mother as son is to: son (0.8) daughter (0.7) mother (0.6) sons (0.5) daughters (0.5) \n",
      "man is to woman as doctor is to: doctor (0.8) doctors (0.6) physician (0.5) woman (0.5) physicians (0.5) \n",
      "germany is to hitler as italy is to: hitler (0.8) mussolini (0.6) italy (0.6) fascism (0.6) italians (0.6) \n",
      "england is to london as germany is to: germany (0.7) london (0.7) berlin (0.6) german (0.5) munich (0.5) \n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    (\"king\", \"queen\", \"man\"),\n",
    "    (\"man\", \"woman\", \"nephew\"),\n",
    "    (\"father\", \"mother\", \"son\"),\n",
    "    (\"man\", \"woman\", \"doctor\"),\n",
    "    (\"germany\", \"hitler\", \"italy\"),\n",
    "    (\"england\", \"london\", \"germany\"),\n",
    "]\n",
    "for (token1, token2, token3) in examples:\n",
    "    print(f\"{token1} is to {token2} as {token3} is to: \", end=\"\")\n",
    "    for similarity, token in find_closest_tokens(token1, token2, token3):\n",
    "        print(f\"{token} ({similarity:.1f})\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the correct answer is generally among the closest. That said, if you play around with other examples, you will find that it only works for fairly simple examples: the embeddings aren't always that simple to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. to 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Stateless RNNs can only capture patterns whose length is less than, or equal to, the size of the windows the RNN is trained on. Conversely, stateful RNNs can capture longer-term patterns. However, implementing a stateful RNN is much harderâ â€”especially preparing the dataset properly. Moreover, stateful RNNs do not always work better, in part because consecutive batches are not independent and identically distributed (IID). Gradient Descent is not fond of non-IID datasets.\n",
    "2. In general, if you translate a sentence one word at a time, the result will be terrible. For example, the French sentence \"Je vous en prie\" means \"You are welcome,\" but if you translate it one word at a time, you get \"I you in pray.\" Huh? It is much better to read the whole sentence first and then translate it. A plain sequence-to-sequence RNN would start translating a sentence immediately after reading the first word, while an Encoderâ€“Decoder RNN will first read the whole sentence and then translate it. That said, one could imagine a plain sequence-to-sequence RNN that would output silence whenever it is unsure about what to say next (just like human translators do when they must translate a live broadcast).\n",
    "3. Variable-length input sequences can be handled by padding the shorter sequences so that all sequences in a batch have the same length, and using masking to ensure the RNN ignores the padding token. For better performance, you may also want to create batches containing sequences of similar sizes. Ragged tensors can hold sequences of variable lengths, and Keras now supports them, which simplifies handling variable-length input sequences (at the time of this writing, it still does not handle ragged tensors as targets on the GPU, though). Regarding variable-length output sequences, if the length of the output sequence is known in advance (e.g., if you know that it is the same as the input sequence), then you just need to configure the loss function so that it ignores tokens that come after the end of the sequence. Similarly, the code that will use the model should ignore tokens beyond the end of the sequence. But generally the length of the output sequence is not known ahead of time, so the solution is to train the model so that it outputs an end-of-sequence token at the end of each sequence.\n",
    "4. Beam search is a technique used to improve the performance of a trained Encoderâ€“Decoder model, for example in a neural machine translation system. The algorithm keeps track of a short list of the _k_ most promising output sentences (say, the top three), and at each decoder step it tries to extend them by one word; then it keeps only the _k_ most likely sentences. The parameter _k_ is called the _beam width_: the larger it is, the more CPU and RAM will be used, but also the more accurate the system will be. Instead of greedily choosing the most likely next word at each step to extend a single sentence, this technique allows the system to explore several promising sentences simultaneously. Moreover, this technique lends itself well to parallelization. You can implement beam search by writing a custom memory cell. Alternatively, TensorFlow Addons's seq2seq API provides an implementation.\n",
    "5. An attention mechanism is a technique initially used in Encoderâ€“Decoder models to give the decoder more direct access to the input sequence, allowing it to deal with longer input sequences. At each decoder time step, the current decoder's state and the full output of the encoder are processed by an alignment model that outputs an alignment score for each input time step. This score indicates which part of the input is most relevant to the current decoder time step. The weighted sum of the encoder output (weighted by their alignment score) is then fed to the decoder, which produces the next decoder state and the output for this time step. The main benefit of using an attention mechanism is the fact that the Encoderâ€“Decoder model can successfully process longer input sequences. Another benefit is that the alignment scores make the model easier to debug and interpret: for example, if the model makes a mistake, you can look at which part of the input it was paying attention to, and this can help diagnose the issue. An attention mechanism is also at the core of the Transformer architecture, in the Multi-Head Attention layers. See the next answer.\n",
    "6. Sampled softmax is used when training a classification model when there are many classes (e.g., thousands). It computes an approximation of the cross-entropy loss based on the logit predicted by the model for the correct class, and the predicted logits for a sample of incorrect words. This speeds up training considerably compared to computing the softmax over all logits and then estimating the cross-entropy loss. After training, the model can be used normally, using the regular softmax function to compute all the class probabilities based on all the logits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work in progress\n",
    "\n",
    "I'm working on the exercise solutions, hoping to finish them by December 2025. Thanks for your patience!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
