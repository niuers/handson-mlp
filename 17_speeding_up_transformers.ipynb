{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 17 ‚Äì Speeding Up Transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains all the sample code and solutions to the exercises in Chapter 17._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ageron/handson-mlp/blob/main/17_advanced_transformer_techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-mlp/blob/main/17_advanced_transformer_techniques.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project requires Python 3.10 or above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we using Colab or Kaggle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need PyTorch ‚â• 2.6.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging.version import Version\n",
    "import torch\n",
    "\n",
    "assert Version(torch.__version__) >= Version(\"2.6.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter can be very slow without a hardware accelerator, so if we can find one, let's use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's issue a warning if there's no hardware accelerator available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cpu\":\n",
    "    print(\"Neural nets can be very slow without a hardware accelerator.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware \"\n",
    "              \"accelerator.\")\n",
    "    if IS_KAGGLE:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in earlier chapters, let's define the default font sizes to make the figures prettier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Decoding at Inference Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key/Value Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404297b41ba54c7bb9963e41c37e5f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20725a7799394075a02930ecbff1071c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1f02ee54b645fea18a169e46fcfce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9744201e45a148329c278ae62cfb6748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5005183ffd5449db8d6532afaccbf8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07565eba848406ea0286d36a7cd2a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6979359c7fef4b8a8f89661ce1afce6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15a04a12bed4bb39b341a2094c46964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cache=True\n",
      "CPU times: user 3.91 s, sys: 150 ms, total: 4.06 s\n",
      "Wall time: 4.62 s\n",
      "use_cache=False\n",
      "CPU times: user 10.6 s, sys: 13.9 ms, total: 10.6 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"facebook/opt-125m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "prompt = \"Once upon a time there lived\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "for use_cache in (True, False):\n",
    "    print(f\"{use_cache=}\")\n",
    "    %time model.generate(**inputs, max_new_tokens=500, do_sample=False, use_cache=use_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speculative decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b2f591a90a4534b3cddc565086f4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c952da6c1c413f979a064893d80b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c60caeb1e4a48d8ba56a28f60613554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/662M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3344c68988bc4d4789479091c13e4c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b6d88ead2f4bc0aac2e56da6551c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0588a68477ba43f1b927588086ee6d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9be9b5a2a2c4ab0a9c5db5fcbf8857d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33eb76d7ec9464ea3149f4fa5acf706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there lived in an aldermanic family the eldest daughter. Her father was a very rich man of considerable wealth.\n",
      "\n",
      "She was very wise and had developed a good marriage with her husband. He never slept on the floor while she slept, but on the table above the bed he never seemed to be awake.\n",
      "\n",
      "The eldest daughter knew what was important to her husband and her parents, and if he refused to do as she told him, she would not accept a roomful of guests for a\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\",\n",
    "                                                    device_map=\"auto\")\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\",\n",
    "                                                   device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "prompt = \"Once upon a time there lived\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(target_model.device)\n",
    "outputs = target_model.generate(**inputs, max_new_tokens=100, do_sample=True,\n",
    "                                temperature=1, assistant_model=draft_model)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BigBird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08052148b58347b5a3f917c4fc0e3dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8a71adc7d84fc68ec7125dba94c9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1f49d79d1f4b7eb5856a8197eb4572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BigBirdForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "BigBirdForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "BigBirdForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753bf3cdd50f4176be2c63f110f72b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dcbf2b68c89443b958eee9e099f02cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/846k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55996e6eaf9a4c11bde79399783a6901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/775 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Attention type 'block_sparse' is not possible if sequence_length: 14 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2818744480609894,\n",
       "  'token': 2457,\n",
       "  'token_str': 'pain',\n",
       "  'sequence': 'She was feeling unwell so she took some pain medicine.'},\n",
       " {'score': 0.21908044815063477,\n",
       "  'token': 4793,\n",
       "  'token_str': 'cold',\n",
       "  'sequence': 'She was feeling unwell so she took some cold medicine.'},\n",
       " {'score': 0.11271445453166962,\n",
       "  'token': 36298,\n",
       "  'token_str': 'allergy',\n",
       "  'sequence': 'She was feeling unwell so she took some allergy medicine.'},\n",
       " {'score': 0.06932126730680466,\n",
       "  'token': 22195,\n",
       "  'token_str': 'cough',\n",
       "  'sequence': 'She was feeling unwell so she took some cough medicine.'},\n",
       " {'score': 0.045178256928920746,\n",
       "  'token': 35968,\n",
       "  'token_str': 'herbal',\n",
       "  'sequence': 'She was feeling unwell so she took some herbal medicine.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"google/bigbird-roberta-base\"\n",
    "pipeline = pipeline(task=\"fill-mask\", model=model_id)\n",
    "pipeline(\"She was feeling unwell so she took some [MASK] medicine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can safely ignore the warnings above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def angular_lsh(vectors, k):\n",
    "    R = torch.randn(vectors.size(-1), k // 2, device=vectors.device)\n",
    "    normalized_vectors = F.normalize(vectors, p=2.0, dim=1)\n",
    "    V_proj = normalized_vectors @ R\n",
    "    V_concat = torch.cat([V_proj, -V_proj], dim=1)\n",
    "    return torch.argmax(V_concat, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 0, 3, 0, 2, 2, 2, 2, 1, 1, 3, 3, 1, 2, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "vectors = torch.rand(16, 512)\n",
    "angular_lsh(vectors, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected value of $\\exp(\\mathbf{w}\\cdot\\mathbf{x})$ for a given vector **x** and a random vector **w** sampled from a Gaussian distribution is $\\exp\\left(\\frac{1}{2}\\|x\\|^2\\right)$. We can test this using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5851, 1.6703, 1.6728, 1.9729, 1.7934])\n",
      "tensor([1.6516, 1.7578, 1.6594, 1.8050, 1.7751])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "d, m = 64, 1024\n",
    "W = torch.randn(d, m)\n",
    "X = torch.randn(5, d) / d ** 0.5\n",
    "R = torch.exp(X @ W)\n",
    "print(R.mean(axis=-1))\n",
    "print(torch.exp(0.5 * (X.norm(dim=-1)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty close!\n",
    "\n",
    "Now let's implement the function $\\phi(\\mathbf{x}) = \\dfrac{\\exp(\\mathbf{x} \\mathbf{W})-\\frac{1}{2}\\|\\mathbf{x}\\|^2)}{\\sqrt m}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(X, W, dim_subtract_max=(-2, -1)):\n",
    "    squared_norms = X.square().sum(dim=-1, keepdim=True)\n",
    "    X_proj = X @ W\n",
    "    max_vals = X_proj.amax(dim=dim_subtract_max, keepdim=True)\n",
    "    return torch.exp(X_proj - max_vals - squared_norms / 2) / W.size(-1) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to prove that the expected value of _œï_(**Q**)_œï_(**K**)<sup>‚ä∫</sup> is equal to exp(**QK**<sup>‚ä∫</sup>). Let's check that this is indeed the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0171)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 32\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "Lq = 200\n",
    "Lk = 100\n",
    "m = 256\n",
    "d_head = d_model // n_heads\n",
    "W = torch.randn(n_heads, d_head, m)\n",
    "Q = torch.randn(batch_size, n_heads, Lq, d_head)\n",
    "K = torch.randn(batch_size, n_heads, Lk, d_head)\n",
    "scale = 1 / d_head ** 0.5\n",
    "Qp = phi(Q * scale ** 0.5, W, dim_subtract_max=-1)\n",
    "Kp = phi(K * scale ** 0.5, W)\n",
    "A = Qp @ Kp.transpose(-2, -1)\n",
    "D = A.sum(dim=-1, keepdim=True)\n",
    "result = A / (D + 1e-6)\n",
    "expected = torch.softmax(Q @ K.transpose(-2, -1) * scale, dim=-1)\n",
    "rmse = F.mse_loss(result, expected) ** 0.5\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty good approximation! We can still improve it by orthogonalizing **W** using QR decomposition. Since there can only _d_ orthogonal vectors in a _d_-dimensional space, we orthogonalize each chunk of _d_ random vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonalize(W):\n",
    "    d_head = W.size(-2)\n",
    "    W_orth = torch.cat([torch.linalg.qr(W_chunk)[0]\n",
    "                        for W_chunk in W.split(d_head, dim=-1)], dim=-1)\n",
    "    return W_orth * d_head ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_orth = orthogonalize(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the RMSE once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0160)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qp2 = phi(Q * scale ** 0.5, W_orth, dim_subtract_max=-1)\n",
    "Kp2 = phi(K * scale ** 0.5, W_orth)\n",
    "A2 = Qp2 @ Kp2.transpose(-2, -1)\n",
    "D2 = A2.sum(dim=-1, keepdim=True)\n",
    "result2 = A2 / (D2 + 1e-6)\n",
    "rmse2 = F.mse_loss(result2, expected) ** 0.5\n",
    "rmse2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a bit better. If you increase the number of features, it will reduce this error further, at the cost of more compute and memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to implement FAVOR+ attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FavorAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, n_features):\n",
    "        super().__init__()\n",
    "        self.d_head = d_model // n_heads\n",
    "        W = torch.randn(n_heads, self.d_head, n_features)  # h, d, m\n",
    "        W = orthogonalize(W)\n",
    "        self.register_buffer(\"W\", W)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        scale = self.d_head ** -0.25\n",
    "        Qp = phi(Q * scale, self.W, dim_subtract_max=-1)\n",
    "        Kp = phi(K * scale, self.W)\n",
    "        D = Qp @ Kp.sum(dim=-2).unsqueeze(-1)  # B, h, Lq, 1\n",
    "        Kp_T_V = Kp.transpose(-2, -1) @ V  # B, h, m, d\n",
    "        return (Qp @ Kp_T_V) / (D + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "Q = torch.randn(batch_size, n_heads, Lq, d_head)\n",
    "K = torch.randn(batch_size, n_heads, Lk, d_head)\n",
    "V = torch.randn(batch_size, n_heads, Lk, d_head)\n",
    "favor_attn = FavorAttention(d_model, n_heads, 256)\n",
    "approx_attn = favor_attn(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1599)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attn = F.scaled_dot_product_attention(Q, K, V)\n",
    "attn_rmse = F.mse_loss(approx_attn, attn) ** 0.5\n",
    "attn_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Projections in Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, Lq, Lk, d_head = 32, 100, 90, 64\n",
    "n_heads = 8\n",
    "n_groups = 1\n",
    "query = torch.randn(batch_size, n_heads, Lq, d_head)\n",
    "key = torch.randn(batch_size, n_groups, Lk, d_head)\n",
    "value = torch.randn(batch_size, n_groups, Lk, d_head)\n",
    "attn = F.scaled_dot_product_attention(query, key, value, enable_gqa=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, Lq, Lk, d_head = 32, 100, 90, 64\n",
    "n_heads = 8\n",
    "n_groups = 2\n",
    "query = torch.randn(batch_size, n_heads, Lq, d_head)\n",
    "key = torch.randn(batch_size, n_groups, Lk, d_head)\n",
    "value = torch.randn(batch_size, n_groups, Lk, d_head)\n",
    "attn = F.scaled_dot_product_attention(query, key, value, enable_gqa=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FlashAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a toy Python implementation of FlashAttention, to get an idea of how it works under the hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention(Q, K, V, block_size_q, block_size_k):\n",
    "    Lq, d = Q.shape\n",
    "    Lk, _ = K.shape\n",
    "    O = torch.zeros_like(Q)\n",
    "    scale = d ** -0.5\n",
    "\n",
    "    for i_start in range(0, Lq, block_size_q):  # iterate over query blocks\n",
    "        i_end = min(i_start + block_size_q, Lq)\n",
    "        Q_block = Q[i_start:i_end]\n",
    "\n",
    "        O_block = torch.zeros(block_size_q, d, device=Q.device)\n",
    "        l_block = torch.zeros(block_size_q, 1, device=Q.device)\n",
    "        m_block = -torch.inf * torch.ones(block_size_q, 1, device=Q.device)\n",
    "\n",
    "        for j_start in range(0, Lk, block_size_k):  # iterate over K/V blocks\n",
    "            j_end = min(j_start + block_size_k, Lk)\n",
    "            K_block = K[j_start:j_end]\n",
    "            V_block = V[j_start:j_end]\n",
    "\n",
    "            # Core attention calculation for the block\n",
    "            S_ij = Q_block @ K_block.T * scale  # (block_size_q, block_size_k)\n",
    "\n",
    "            # Find the new maximum score for the combined blocks so far\n",
    "            m_ij_new, _ = torch.max(S_ij, dim=1, keepdim=True)\n",
    "            m_block_new = torch.maximum(m_block, m_ij_new)\n",
    "\n",
    "            # Rescale previous accumulator values based on the new max\n",
    "            P_ij = torch.exp(S_ij - m_block_new)\n",
    "            correction_factor = torch.exp(m_block - m_block_new)\n",
    "\n",
    "            # Update the denominator (l) and output (O) accumulators\n",
    "            l_block_new = ((l_block * correction_factor)\n",
    "                           + torch.sum(P_ij, dim=1, keepdim=True))\n",
    "            O_block = (O_block * correction_factor) + (P_ij @ V_block)\n",
    "\n",
    "            # Update the state for the next inner loop iteration\n",
    "            l_block = l_block_new\n",
    "            m_block = m_block_new\n",
    "\n",
    "        # Rescale the output block and write it to the final output matrix\n",
    "        O[i_start:i_end] = O_block / l_block\n",
    "\n",
    "    return O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that this works (note that this simple implementation only handles the case where _L_<sub>Q</sub> and _L_<sub>K</sub> are multiples of the block size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "block_size_q, block_size_k = 64, 64\n",
    "Lq, Lk, d = 1280, 1152, 512\n",
    "Q = torch.randn(Lq, d)\n",
    "K = torch.randn(Lk, d)\n",
    "V = torch.randn(Lk, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "R1 = flash_attention(Q, K, V, block_size_q, block_size_k)\n",
    "R2 = F.scaled_dot_product_attention(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.0755e-16)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = F.mse_loss(R1, R2)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Up With Mixture of experts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82e9b30d19644718dd540a06139e721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0416cfa5772c41ab909323cf7b2efb84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c13ba72586419eb09de7ece18e9a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3a9e632fbb408a9e9b9d13cbf6c206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f082cafd75f0410e9ef8f835f3b3b573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82218feb3bb6450caf95324a1e040361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c35a99245a942a4967dd283465c185f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da964699062e45e7bf10d5468ae53e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 589,824 || all params: 125,788,416 || trainable%: 0.4689\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model_id = \"EleutherAI/gpt-neo-125M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\",\n",
    "                                             dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (TrainingArguments, Trainer,\n",
    "                          DataCollatorForLanguageModeling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(10, 1).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "data_loader = [(torch.randn(8, 10), torch.randn(8, 1)) for _ in range(100)]\n",
    "model.train()\n",
    "\n",
    "accumulation_steps = 4\n",
    "optimizer.zero_grad()  # reset gradients before starting\n",
    "for batch_index, (X_batch, y_batch) in enumerate(data_loader):\n",
    "    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "    y_pred = model(X_batch)\n",
    "    loss = criterion(y_pred, y_batch)\n",
    "    loss = loss / accumulation_steps\n",
    "    loss.backward()\n",
    "    if (batch_index + 1) % accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work in progress\n",
    "\n",
    "I'm working on the exercise solutions, hoping to finish them by December 2025. Thanks for your patience!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
